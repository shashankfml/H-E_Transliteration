{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, string, glob\n",
    "import random, math, time\n",
    "\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preparation():\n",
    "    def __init__(self, DATAPATH):\n",
    "        self.train_path = glob.glob(DATAPATH + '/*')[1]\n",
    "        self.val_path = glob.glob(DATAPATH + '/*')[2]\n",
    "        self.test_path = glob.glob(DATAPATH + '/*')[0]\n",
    "\n",
    "        self.train_df = pd.read_csv(self.train_path, names = ['English', 'Hindi'])\n",
    "        self.val_df = pd.read_csv(self.val_path, names = ['English', 'Hindi'])\n",
    "        self.test_df = pd.read_csv(self.test_path, names = ['English', 'Hindi'])\n",
    "\n",
    "\n",
    "    \n",
    "    def dictionary_lookup(self, vocab):\n",
    "        char2int = dict([(char, i) for i, char in enumerate(vocab)])\n",
    "        int2char = dict([(i, char) for char, i in char2int.items()])\n",
    "        return char2int, int2char\n",
    "    \n",
    "    def encode(self, source, target, source_chars, target_chars, source_char2int=None, target_char2int=None):\n",
    "        num_encoder_tokens = len(source_chars)\n",
    "        num_decoder_tokens = len(target_chars)\n",
    "        max_source_length = max([len(txt) for txt in source])\n",
    "        max_target_length = max([len(txt) for txt in target])\n",
    "\n",
    "        encoder_input_data = np.zeros((len(source), max_source_length, num_encoder_tokens), dtype=\"float32\")\n",
    "        decoder_input_data = np.zeros((len(target), max_target_length, num_decoder_tokens), dtype=\"float32\")\n",
    "        decoder_target_data = np.zeros((len(target), max_target_length, num_decoder_tokens), dtype=\"float32\")\n",
    "\n",
    "        source_vocab, target_vocab = None, None\n",
    "        if source_char2int == None and target_char2int == None:\n",
    "            \n",
    "            source_char2int, source_int2char = self.dictionary_lookup(source_chars)\n",
    "            target_char2int, target_int2char = self.dictionary_lookup(target_chars)\n",
    "\n",
    "            source_vocab = (source_char2int, source_int2char)\n",
    "            target_vocab = (target_char2int, target_int2char)\n",
    "\n",
    "        for i, (input_text, target_text) in enumerate(zip(source, target)):\n",
    "            for t, char in enumerate(input_text):\n",
    "                encoder_input_data[i, t, source_char2int[char]] = 1.0\n",
    "            encoder_input_data[i, t + 1 :,source_char2int[\"-PAD-\"]] = 1.0\n",
    "            for t, char in enumerate(target_text):\n",
    "                # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "                decoder_input_data[i, t, target_char2int[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    # and will not include the start character.\n",
    "                    decoder_target_data[i, t - 1, target_char2int[char]] = 1.0\n",
    "            decoder_input_data[i, t + 1:, target_char2int[\"-PAD-\"]] = 1.0\n",
    "            decoder_target_data[i, t:, target_char2int[\"-PAD-\"]] = 1.0\n",
    "        \n",
    "        if source_vocab !=None and target_vocab !=None:\n",
    "            return encoder_input_data, decoder_input_data, decoder_target_data, source_vocab, target_vocab\n",
    "\n",
    "        else:\n",
    "            return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "        \n",
    "\n",
    "    def preprocess(self, source, target):\n",
    "        source_chars = set(list(string.ascii_lowercase))\n",
    "        #source_chars = set()\n",
    "        target_chars = set([chr(alpha) for alpha in range(2304, 2432)])\n",
    "\n",
    "        source = [str(x) for x in source]\n",
    "        target = [str(x) for x in target]\n",
    "\n",
    "        source_words = []\n",
    "        target_words = []\n",
    "        for src, tgt in zip(source, target):\n",
    "            tgt = \"\\t\" + tgt + \"\\n\"\n",
    "            source_words.append(src)\n",
    "            target_words.append(tgt)\n",
    "            for char in src:\n",
    "                if char not in source_chars:\n",
    "                    source_chars.add(char)\n",
    "            for char in tgt:\n",
    "                if char not in target_chars:\n",
    "                    target_chars.add(char)\n",
    "        \n",
    "        source_chars = sorted(list(source_chars))\n",
    "        target_chars = sorted(list(target_chars))\n",
    "\n",
    "        # Adding pad token\n",
    "        source_chars.append('-PAD-')\n",
    "        target_chars.append('-PAD-')\n",
    "\n",
    "        self.num_encoder_tokens = len(source_chars)\n",
    "        self.num_decoder_tokens = len(target_chars)\n",
    "        self.max_source_length = max([len(txt) for txt in source_words])\n",
    "        self.max_target_length = max([len(txt) for txt in target_words])\n",
    "\n",
    "        print(\"\\n Number of samples:\", len(source))\n",
    "        print(\"Source Vocab length:\", self.num_encoder_tokens)\n",
    "        print(\"Target Vocab length:\", self.num_decoder_tokens)\n",
    "        print(\"Max sequence length for inputs:\", self.max_source_length)\n",
    "        print(\"Max sequence length for outputs:\", self.max_target_length)\n",
    "        \n",
    "        return source_words, target_words, source_chars, target_chars\n",
    "    \n",
    "    def create_dataloaders(self, batch_size):\n",
    "        train_source_words, train_target_words, train_source_chars, train_target_chars = self.preprocess(self.train_df[\"English\"].to_list(), self.train_df[\"Hindi\"].to_list())\n",
    "        self.train_data = self.encode(train_source_words, train_target_words, train_source_chars, train_target_chars)\n",
    "        (self.train_encoder_input, self.train_decoder_input, self.train_decoder_target, self.source_vocab, self.target_vocab) = self.train_data\n",
    "        self.source_char2int, self.source_int2char = self.source_vocab\n",
    "        self.target_char2int, self.target_int2char = self.target_vocab\n",
    "\n",
    "        val_source_words, val_target_words, val_source_chars, val_target_chars  = self.preprocess(self.val_df[\"English\"].to_list(), self.val_df[\"Hindi\"].to_list())\n",
    "        self.val_data = self.encode(val_source_words, val_target_words, list(self.source_char2int.keys()), list(self.target_char2int.keys()), source_char2int = self.source_char2int, target_char2int=self.target_char2int)\n",
    "        self.val_encoder_input, self.val_decoder_input, self.val_decoder_target = self.val_data\n",
    "        # self.source_char2int, self.source_int2char = self.source_vocab\n",
    "        # self.target_char2int, self.target_int2char = self.target_vocab\n",
    "\n",
    "        test_source_words, test_target_words, test_source_chars, test_target_chars  = self.preprocess(self.test_df[\"English\"].to_list(), self.test_df[\"Hindi\"].to_list())\n",
    "        self.test_data = self.encode(test_source_words, test_target_words, list(self.source_char2int.keys()), list(self.target_char2int.keys()), source_char2int = self.source_char2int, target_char2int=self.target_char2int)\n",
    "        self.test_encoder_input, self.test_decoder_input, self.test_decoder_target = self.test_data\n",
    "        # self.source_char2int, self.source_int2char = self.source_vocab\n",
    "        # self.target_char2int, self.target_int2char = self.target_vocab\n",
    "\n",
    "        encoder_input_data_train = torch.stack([torch.from_numpy(np.array(i)) for i in self.train_encoder_input])\n",
    "        decoder_input_data_train = torch.stack([torch.from_numpy(np.array(i)) for i in self.train_decoder_input])\n",
    "    \n",
    "        encoder_input_data_val = torch.stack([torch.from_numpy(np.array(i)) for i in self.val_encoder_input])\n",
    "        decoder_input_data_val = torch.stack([torch.from_numpy(np.array(i)) for i in self.val_decoder_input])\n",
    "\n",
    "        encoder_input_data_test = torch.stack([torch.from_numpy(np.array(i)).float() for i in self.test_encoder_input])\n",
    "        decoder_input_data_test = torch.stack([torch.from_numpy(np.array(i)).float() for i in self.test_decoder_input])\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(encoder_input_data_train, decoder_input_data_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = torch.utils.data.TensorDataset(encoder_input_data_val, decoder_input_data_val)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        test_dataset = torch.utils.data.TensorDataset(encoder_input_data_test, decoder_input_data_test)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note:** Model Building part has been inspired from below link\n",
    "\n",
    "'https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Linear layer to combine hidden state and encoder outputs\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        # Parameter vector for attention\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        # Initialize parameter vector\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate attention energies\n",
    "        attn_energies = self.score(hidden, encoder_outputs)\n",
    "        # Return softmax normalized probabilities\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Calculate the attention score for each encoder output.\n",
    "        \"\"\"\n",
    "        # Repeat hidden state across all time steps\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        # Transpose encoder outputs for concatenation\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        # Concatenate hidden state and encoder outputs and pass through a linear layer\n",
    "        energy = F.relu(self.attn(torch.cat([h, encoder_outputs], 2)))\n",
    "        # Transpose energy for batch multiplication\n",
    "        energy = energy.transpose(1, 2)\n",
    "        # Repeat and reshape the parameter vector for batch multiplication\n",
    "        v = self.v.unsqueeze(0).expand(encoder_outputs.size(0), -1).unsqueeze(1)\n",
    "        # Compute the attention scores\n",
    "        energy = torch.bmm(v, energy)\n",
    "        return energy.squeeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_encoder_tokens, hidden_dim, n_layers, dropout, encoder_embedding_dim=0, cell_type=\"LSTM\", verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the Encoder.\n",
    "\n",
    "        Parameters:\n",
    "        num_encoder_tokens (int): Size of the input vocabulary.\n",
    "        hidden_dim (int): Number of features in the hidden state.\n",
    "        n_layers (int): Number of recurrent layers.\n",
    "        dropout (float): Dropout probability.\n",
    "        encoder_embedding_dim (int, optional): Dimension of the embeddings (0 if not using embeddings). Default is 0.\n",
    "        cell_type (str, optional): Type of RNN cell ('LSTM', 'GRU', or 'RNN'). Default is 'LSTM'.\n",
    "        verbose (bool, optional): If True, print shapes of tensors for debugging. Default is False.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.embedding_dim = encoder_embedding_dim\n",
    "        self.encoder_input_size = num_encoder_tokens\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Adjust dropout for single-layer RNNs (no dropout if n_layers == 1)\n",
    "        dropout = 0 if (n_layers == 1) else dropout\n",
    "\n",
    "        # If embedding dimension is specified, add an embedding layer\n",
    "        if self.embedding_dim != 0:\n",
    "            self.encoder_input_size = self.embedding_dim\n",
    "            self.embedding = nn.Embedding(num_encoder_tokens, self.embedding_dim, padding_idx=num_encoder_tokens-1)\n",
    "\n",
    "        # Define the RNN cell (LSTM, GRU, or RNN)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(self.encoder_input_size, hidden_dim, n_layers, dropout=dropout)\n",
    "        elif self.cell_type == 'RNN':\n",
    "            self.rnn = nn.RNN(self.encoder_input_size, hidden_dim, n_layers, dropout=dropout)\n",
    "        elif self.cell_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(self.encoder_input_size, hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Parameters:\n",
    "        input (Tensor): Input tensor of shape (batch_size, seq_length, num_encoder_tokens).\n",
    "\n",
    "        Returns:\n",
    "        outputs (Tensor): Output features from the last layer of the RNN for each time step of shape (seq_length, batch_size, hidden_dim).\n",
    "        hidden_cell (Tensor or Tuple[Tensor, Tensor]): Hidden state (and cell state if LSTM) of shape (n_layers, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        # Transpose input to have sequence length first\n",
    "        input = input.transpose(0, 1)\n",
    "\n",
    "        # If embedding is used, apply embedding and dropout\n",
    "        if self.embedding_dim != 0:\n",
    "            input = input.argmax(2)  # Convert one-hot encoding to indices\n",
    "            input = self.dropout(self.embedding(input))  # Apply embedding and dropout\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input shape after embedding: {input.shape}\")\n",
    "\n",
    "        # Pass the input through the RNN\n",
    "        outputs, hidden_cell = self.rnn(input)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Input shape: {input.shape}\")\n",
    "            print(f\"Outputs shape: {outputs.shape}\")\n",
    "            print(f\"Hidden/Cell state shape: {hidden_cell.shape}\")\n",
    "\n",
    "        return outputs, hidden_cell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_decoder_tokens, decoder_hidden_dim, n_layers, dropout, decoder_embedding_dim=0, cell_type='LSTM', atten=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Decoder module of a seq2seq model.\n",
    "\n",
    "        Args:\n",
    "        - num_decoder_tokens (int): Number of tokens in the decoder vocabulary.\n",
    "        - decoder_hidden_dim (int): Dimensionality of the decoder hidden states.\n",
    "        - n_layers (int): Number of layers in the decoder RNN.\n",
    "        - dropout (float): Dropout probability.\n",
    "        - decoder_embedding_dim (int, optional): Dimensionality of the decoder embeddings. Defaults to 0.\n",
    "        - cell_type (str, optional): Type of RNN cell used in the decoder ('LSTM', 'GRU', or 'RNN'). Defaults to 'LSTM'.\n",
    "        - atten (bool, optional): Whether to use attention mechanism. Defaults to False.\n",
    "        - verbose (bool, optional): Whether to print verbose information during forward pass. Defaults to False.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = num_decoder_tokens\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.cell_type = cell_type\n",
    "        self.attention = atten\n",
    "        self.n_layers = n_layers\n",
    "        self.decoder_embedding_dim = decoder_embedding_dim\n",
    "        self.decoder_input = num_decoder_tokens\n",
    "        self.verbose = verbose\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Adjust dropout for single-layer RNNs\n",
    "        dropout = 0 if (n_layers == 1) else dropout\n",
    "\n",
    "        if self.decoder_hidden_dim != 0:\n",
    "            self.decoder_input = self.decoder_embedding_dim\n",
    "            self.embedding = nn.Embedding(num_decoder_tokens, self.decoder_embedding_dim)\n",
    "\n",
    "        if self.attention == False:\n",
    "            if cell_type == 'LSTM':\n",
    "                self.rnn = nn.LSTM(self.decoder_input, self.decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "            elif cell_type == 'RNN':\n",
    "                self.rnn = nn.RNN(self.decoder_input, self.decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "            elif cell_type == 'GRU':\n",
    "                self.rnn = nn.GRU(self.decoder_input, self.decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "            self.fc_out = nn.Linear(self.decoder_hidden_dim, self.output_dim)\n",
    "\n",
    "        else:\n",
    "            self.attention = Attention(self.decoder_hidden_dim)\n",
    "            \n",
    "            if cell_type == \"LSTM\":\n",
    "                self.rnn = nn.LSTM(self.decoder_hidden_dim + self.decoder_input, decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "            elif cell_type == \"RNN\":\n",
    "                self.rnn = nn.RNN(self.decoder_hidden_dim + self.decoder_input, decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "            elif cell_type == \"GRU\":\n",
    "                self.rnn = nn.GRU(self.decoder_hidden_dim + self.decoder_input, decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "            self.fc_out = nn.Linear(self.decoder_hidden_dim * 2, self.output_dim)\n",
    "    \n",
    "    def forward(self, input, hidden_cell, encoder_states):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "\n",
    "        Args:\n",
    "        - input (Tensor): Input tensor of shape (batch_size, 1, num_decoder_tokens).\n",
    "        - hidden_cell (Tensor or Tuple[Tensor, Tensor]): Hidden state (and cell state if LSTM) from the encoder.\n",
    "        - encoder_states (Tensor): Encoder outputs of shape (seq_length, batch_size, hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "        - output (Tensor): Output tensor of shape (batch_size, num_decoder_tokens).\n",
    "        - hidden (Tensor or Tuple[Tensor, Tensor]): Hidden state (and cell state if LSTM) of shape (n_layers, batch_size, hidden_dim).\n",
    "        - attn_weights (Tensor, optional): Attention weights of shape (batch_size, 1, seq_length) if attention is used.\n",
    "        \"\"\"\n",
    "        if isinstance(hidden_cell, tuple):\n",
    "            hidden = hidden_cell[0]\n",
    "            cell = hidden_cell[1]\n",
    "        else:\n",
    "            hidden = hidden_cell\n",
    "        \n",
    "        if self.decoder_embedding_dim != 0:\n",
    "            input = input.argmax(2)\n",
    "            input = self.dropout(self.embedding(input))\n",
    "\n",
    "        if self.attention == False:\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                output, hidden = self.rnn(input, (hidden, cell))\n",
    "            else:\n",
    "                output, hidden = self.rnn(input, hidden)\n",
    "\n",
    "            prediction = self.fc_out(output.squeeze(0))\n",
    "\n",
    "            return prediction, hidden\n",
    "        \n",
    "        else:\n",
    "            attn_weights = self.attention(hidden[-1], encoder_states)\n",
    "            context = attn_weights.bmm(encoder_states.transpose(0, 1))\n",
    "            context = context.transpose(0, 1)\n",
    "            rnn_input = torch.cat([input, context], 2)\n",
    "\n",
    "            if self.cell_type == \"LSTM\":\n",
    "                output, hidden = self.rnn(rnn_input, (hidden, cell))\n",
    "            else:\n",
    "                output, hidden = self.rnn(rnn_input, hidden)\n",
    "            \n",
    "            output = output.squeeze(0)\n",
    "            context = context.squeeze(0)\n",
    "            output = self.fc_out(torch.cat([output, context], 1))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(output.shape)\n",
    "                print(hidden.shape)\n",
    "\n",
    "            return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, max_source_length, max_target_length, target_char2int, num_decoder_tokens, device):\n",
    "        \"\"\"\n",
    "        Sequence-to-Sequence model composed of an encoder and a decoder.\n",
    "\n",
    "        Args:\n",
    "        - encoder (nn.Module): Encoder module.\n",
    "        - decoder (nn.Module): Decoder module.\n",
    "        - max_source_length (int): Maximum length of source sequences.\n",
    "        - max_target_length (int): Maximum length of target sequences.\n",
    "        - target_char2int (dict): Mapping from target characters to integers.\n",
    "        - num_decoder_tokens (int): Number of tokens in the decoder vocabulary.\n",
    "        - device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.atten = self.decoder.attention\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.target_chr2int = target_char2int\n",
    "        self.num_decoder_tokens = num_decoder_tokens\n",
    "\n",
    "        assert encoder.hidden_dim == decoder.decoder_hidden_dim, \"Hidden dimensions of encoder and decoder must be equal\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers\"\n",
    "        \n",
    "    def forward(self, src, trg, to_train, teacher_forcing_ratio=0.5, beam_width=3):\n",
    "        \"\"\"\n",
    "        Forward pass of the seq2seq model.\n",
    "\n",
    "        Args:\n",
    "        - src (Tensor): Source sequence tensor of shape (max_source_length, batch_size).\n",
    "        - trg (Tensor): Target sequence tensor of shape (max_target_length, batch_size).\n",
    "        - to_train (bool): Whether to train the model or not.\n",
    "        - teacher_forcing_ratio (float): Probability of teacher forcing during training.\n",
    "        - beam_width (int): Beam width for beam search decoding.\n",
    "\n",
    "        Returns:\n",
    "        - outputs (Tensor): Model outputs of shape (max_target_length, batch_size, num_decoder_tokens).\n",
    "        \"\"\"\n",
    "        if to_train:\n",
    "            teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        else:\n",
    "            teacher_forcing_ratio = 0\n",
    "\n",
    "        trg = trg.transpose(0, 1)\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_output, hidden_cell = self.encoder(src)\n",
    "\n",
    "        inp = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            if self.atten == False:\n",
    "                prediction, hidden_cell = self.decoder(inp.unsqueeze(0), hidden_cell, encoder_output)\n",
    "            else:\n",
    "                prediction, hidden_cell, attn_weights = self.decoder(inp.unsqueeze(0), hidden_cell, encoder_output)\n",
    "\n",
    "            outputs[t] = prediction\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = prediction.argmax(1)\n",
    "            top1_one_hot = torch.zeros_like(prediction).to(self.device)\n",
    "            top1_one_hot[:, top1] = 1.0\n",
    "\n",
    "            inp = trg[t] if teacher_force else top1_one_hot\n",
    "\n",
    "        return outputs\n",
    "\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calc(target, output, train=True):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model predictions.\n",
    "\n",
    "    Args:\n",
    "    - target (Tensor): Target sequences tensor of shape (max_target_length, batch_size).\n",
    "    - output (Tensor): Model output sequences tensor of shape (max_target_length, batch_size, num_decoder_tokens).\n",
    "    - train (bool): Whether the model is being trained or not.\n",
    "\n",
    "    Returns:\n",
    "    - num_correct (int): Number of correctly predicted sequences.\n",
    "    - batch_size (int): Total number of sequences in the batch.\n",
    "    \"\"\"\n",
    "    target = target.transpose(0, 1)\n",
    "    num_correct = 0\n",
    "    batch_size = target.shape[0]\n",
    "\n",
    "    target_indices = (target == 1).nonzero()[:, 1]\n",
    "\n",
    "    assert batch_size == len(target_indices)\n",
    "\n",
    "    if train:\n",
    "        output = output.argmax(2)  # LxB\n",
    "        output = output.transpose(0, 1)\n",
    "        output_indices = (output == 1).nonzero()[:, 1]\n",
    "        for seq, i in zip(range(batch_size), target_indices):\n",
    "            if torch.all(output[seq, :i + 1] == target[seq, :i + 1]):\n",
    "                num_correct += 1\n",
    "    else:\n",
    "        for seq, i in zip(range(batch_size), target_indices):\n",
    "            if torch.all(torch.tensor(output[seq][0]).to(device) == target[seq, :i + 1]):\n",
    "                num_correct += 1\n",
    "\n",
    "    return num_correct, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
    "    \"\"\"\n",
    "    Train the model on the given data.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The model to be trained.\n",
    "    - iterator (DataLoader): Data iterator containing the training data.\n",
    "    - optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - clip (float): Gradient clipping value.\n",
    "    - teacher_forcing_ratio (float): Probability of teacher forcing during training.\n",
    "    - device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - epoch_loss (float): Average loss per epoch.\n",
    "    - accuracy (float): Accuracy of the model on the training data.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    total_no_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg, teacher_forcing_ratio=teacher_forcing_ratio, to_train=True).to(device)\n",
    "        trg = trg.transpose(0, 1)\n",
    "        trg = trg.argmax(2)\n",
    "        num_correct, num_samples = accuracy_calc(trg, output, train=True)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        total_no_correct += num_correct\n",
    "        total_samples += num_samples\n",
    "\n",
    "    return epoch_loss / len(iterator), total_no_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, beam_width, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The model to be evaluated.\n",
    "    - iterator (DataLoader): Data iterator containing the evaluation data.\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - beam_width (int): Width of the beam for beam search.\n",
    "    - device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    - epoch_loss (float): Average loss per epoch.\n",
    "    - accuracy (float): Accuracy of the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_no_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0, to_train=True).to(device)\n",
    "            trg = trg.transpose(0, 1)\n",
    "            trg = trg.argmax(2)\n",
    "            num_correct, num_samples = accuracy_calc(trg, output, train=True)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].reshape(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            total_no_correct += num_correct\n",
    "            total_samples += num_samples\n",
    "\n",
    "    return epoch_loss / len(iterator), total_no_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataloader, validation_dataloader, device, criterion, config, SAVE_PATH, clip=1, sweep = True):\n",
    "    \"\"\"\n",
    "    Train the model using the training data and evaluate it on the validation data.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The model to be trained and evaluated.\n",
    "    - train_dataloader (DataLoader): DataLoader containing the training data.\n",
    "    - validation_dataloader (DataLoader): DataLoader containing the validation data.\n",
    "    - device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "    - criterion (nn.Module): Loss function.\n",
    "    - config (dict): Configuration parameters including optimizer, learning rate, weight decay, etc.\n",
    "    - clip (float): Gradient clipping value (default=1).\n",
    "\n",
    "    Returns:\n",
    "    - epoch_loss_train (list): List of training losses for each epoch.\n",
    "    - epoch_loss_val (list): List of validation losses for each epoch.\n",
    "    - epoch_accuracy_train (list): List of training accuracies for each epoch.\n",
    "    - epoch_accuracy_val (list): List of validation accuracies for each epoch.\n",
    "    \"\"\"\n",
    "    if config.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        #scheduler = StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "        # scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs/2, eta_min = 0.00001)\n",
    "    elif config.optimizer == 'NAdam':\n",
    "        optimizer = torch.optim.NAdam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        #scheduler = StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "        # scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs/2, eta_min = 0.00001)\n",
    "    elif config.optimizer == 'RAdam':\n",
    "        optimizer = torch.optim.RAdam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        #scheduler = StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "        # scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs/2, eta_min = 0.00001)\n",
    "    elif config.optimizer == 'AdamW':\n",
    "        optimizer = torch.optim.RAdam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        #scheduler = StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "        # scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs/2, eta_min = 0.00001)\n",
    "\n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_val = []\n",
    "    epoch_accuracy_train = []\n",
    "    epoch_accuracy_val = []\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        print(f\"\\n EPOCH: {epoch}\")\n",
    "\n",
    "        train_loss, train_accuracy = train(model, train_dataloader, optimizer, criterion, clip, config.teacher_forcing_ratio, device)\n",
    "        val_loss, val_accuracy = evaluate(model, validation_dataloader, criterion, config.beam_width, device)\n",
    "\n",
    "        if sweep:\n",
    "            wandb.log({\"validation_accuracy\": val_accuracy, \"validation_loss\": val_loss, \"training_accuracy\": train_accuracy, \"training_loss\": train_loss, \"epochs\": epoch})\n",
    "\n",
    "        epoch_loss_train.append(train_loss)\n",
    "        epoch_loss_val.append(val_loss)\n",
    "        epoch_accuracy_train.append(train_accuracy)\n",
    "        epoch_accuracy_val.append(val_accuracy)\n",
    "\n",
    "        print(f\"TRAINING LOSS :{train_loss}\")\n",
    "        print(f\"TRAINING ACCURACY :{train_accuracy}\")\n",
    "        print(f\"VALIDATION LOSS :{val_loss}\")\n",
    "        print(f\"VALIDATION ACCURACY :{val_accuracy}\")\n",
    "\n",
    "        #scheduler.step()\n",
    "    \n",
    "    torch.save(model, os.path.join(SAVE_PATH + '/model.pth'))\n",
    "    if not sweep:\n",
    "        return epoch_loss_train, epoch_loss_val, epoch_accuracy_train, epoch_accuracy_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sweep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train(sweep_config=None):\n",
    "    user = \"Shashank M\"\n",
    "    project = \"Assignment_3_trial\"\n",
    "    display_name = \"ch23s019\"\n",
    "    wandb.init(entity=user, project=project, name=display_name, config = sweep_config)\n",
    "\n",
    "    config_ = wandb.config\n",
    "    wandb.run.name = \"_cell_type_\" + str(config_.cell_type) + \"__embedding__\" + str(config_.encoder_embedding_dim) + \"__hidden__\" + str(config_.hidden_dim) + \"__attention__\" + str(config_.attention) + \"lr_\" + str(config_.learning_rate) + \"_opt_\" + str(config_.optimizer) + \"_epoch_\" + str(config_.epochs) + \"_bs_\" + str(config_.batch_size) \n",
    "\n",
    "    # LOAD DATASET\n",
    "\n",
    "    DATAPATH = '/home/fml-pc/Assignments/Assignment_3/Assignment_3_2024/aksharantar_sampled/aksharantar_sampled/hin'\n",
    "    dataset_func = Data_Preparation(DATAPATH)\n",
    "    train_dataloader, validation_dataloader, test_dataloader = dataset_func.create_dataloaders(config_.batch_size)\n",
    "\n",
    "    num_encoder_tokens = dataset_func.num_encoder_tokens\n",
    "    hidden_dim = config_.hidden_dim\n",
    "    n_layers = config_.n_layers\n",
    "    encoder_embedding_dim = config_.encoder_embedding_dim\n",
    "    dropout = config_.dropout\n",
    "    cell_type = config_.cell_type\n",
    "    decoder_embedding_dim = config_.encoder_embedding_dim\n",
    "    num_decoder_tokens = dataset_func.num_decoder_tokens\n",
    "    \n",
    "    attention = config_.attention\n",
    "\n",
    "    device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    enc = Encoder(num_encoder_tokens, hidden_dim, n_layers, dropout, encoder_embedding_dim, cell_type, verbose=False)\n",
    "    dec = Decoder(num_decoder_tokens, hidden_dim, n_layers, dropout, decoder_embedding_dim, cell_type, atten=attention, verbose=False)\n",
    "    model = Seq2Seq(enc, dec, dataset_func.max_source_length, dataset_func.max_target_length, dataset_func.target_char2int, dataset_func.num_decoder_tokens, device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    SAVE_PATH = '/home/fml-pc/Assignments/Assignment_3/Assignment_3_2024/'\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = dataset_func.target_char2int[\"-PAD-\"])\n",
    "    train_loop(model, train_dataloader, validation_dataloader, device, criterion,config=config_, SAVE_PATH= SAVE_PATH, clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric':{\n",
    "        'name' : 'val_accuracy',\n",
    "        'goal' : 'maximize'},\n",
    "    'parameters':{\n",
    "        'epochs':{'values': [25, 30, 40]},\n",
    "        'cell_type':{'values':['GRU', 'LSTM']},\n",
    "        'n_layers':{'values':[1, 2, 3]},\n",
    "        'hidden_dim':{'values':[256, 400, 512, 1024]},\n",
    "        'encoder_embedding_dim':{'values':[200, 256, 300, 512]},\n",
    "        'dropout':{'values':[0.2, 0.4, 0.5]},\n",
    "        'teacher_forcing_ratio':{'values':[0.3, 0.35, 0.4, 0.45, 0.5]},\n",
    "        'learning_rate': {'min': 0.0001, 'max': 0.001},\n",
    "        'optimizer':{'values': ['Adam', 'NAdam', 'RAdam', 'AdamW']},\n",
    "        'batch_size': {'values':[64, 128, 256]},\n",
    "        'weight_decay':{'values':[0]},\n",
    "        'attention':{'values':[True]},\n",
    "        'beam_width': {'values': [1, 2, 3]}        \n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'bayes', 'metric': {'name': 'val_accuracy', 'goal': 'maximize'}, 'parameters': {'epochs': {'values': [25, 30, 40]}, 'cell_type': {'values': ['GRU', 'LSTM']}, 'n_layers': {'values': [1, 2, 3]}, 'hidden_dim': {'values': [256, 400, 512, 1024]}, 'encoder_embedding_dim': {'values': [200, 256, 300, 512]}, 'dropout': {'values': [0.2, 0.4, 0.5]}, 'teacher_forcing_ratio': {'values': [0.3, 0.35, 0.4, 0.45, 0.5]}, 'learning_rate': {'min': 0.0001, 'max': 0.001}, 'optimizer': {'values': ['Adam', 'NAdam', 'RAdam', 'AdamW']}, 'batch_size': {'values': [64, 128, 256]}, 'weight_decay': {'values': [0]}, 'attention': {'values': [True]}, 'beam_width': {'values': [1, 2, 3]}}}\n",
      "Create sweep with ID: 0l5hwc4d\n",
      "Sweep URL: https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iku3rg3c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0002533453975210106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: NAdam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.45\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mch23s019\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_162609-iku3rg3c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/iku3rg3c' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/iku3rg3c' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/iku3rg3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.3740765592455864\n",
      "TRAINING ACCURACY :0.00623046875\n",
      "VALIDATION LOSS :1.4574610590934753\n",
      "VALIDATION ACCURACY :0.007080078125\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1313007655739784\n",
      "TRAINING ACCURACY :0.08216796875\n",
      "VALIDATION LOSS :1.0255829989910126\n",
      "VALIDATION ACCURACY :0.116943359375\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8367953509092331\n",
      "TRAINING ACCURACY :0.1634375\n",
      "VALIDATION LOSS :0.9488147888332605\n",
      "VALIDATION ACCURACY :0.14453125\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6911185155063868\n",
      "TRAINING ACCURACY :0.22146484375\n",
      "VALIDATION LOSS :0.9495914299041033\n",
      "VALIDATION ACCURACY :0.13818359375\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6059304965287446\n",
      "TRAINING ACCURACY :0.26490234375\n",
      "VALIDATION LOSS :0.8726418688893318\n",
      "VALIDATION ACCURACY :0.19970703125\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5471530643105507\n",
      "TRAINING ACCURACY :0.2942578125\n",
      "VALIDATION LOSS :0.8401470873504877\n",
      "VALIDATION ACCURACY :0.21728515625\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5069527696073055\n",
      "TRAINING ACCURACY :0.32228515625\n",
      "VALIDATION LOSS :0.8274584375321865\n",
      "VALIDATION ACCURACY :0.235107421875\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4629372343420982\n",
      "TRAINING ACCURACY :0.3519140625\n",
      "VALIDATION LOSS :0.8799200784415007\n",
      "VALIDATION ACCURACY :0.239013671875\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.438704754114151\n",
      "TRAINING ACCURACY :0.3714453125\n",
      "VALIDATION LOSS :0.8594630472362041\n",
      "VALIDATION ACCURACY :0.234130859375\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.41205041147768495\n",
      "TRAINING ACCURACY :0.39392578125\n",
      "VALIDATION LOSS :0.8628560993820429\n",
      "VALIDATION ACCURACY :0.25390625\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3919555037096143\n",
      "TRAINING ACCURACY :0.4164453125\n",
      "VALIDATION LOSS :0.8859615046530962\n",
      "VALIDATION ACCURACY :0.23388671875\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3646499811857939\n",
      "TRAINING ACCURACY :0.43677734375\n",
      "VALIDATION LOSS :0.9104274790734053\n",
      "VALIDATION ACCURACY :0.240966796875\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3331635319441557\n",
      "TRAINING ACCURACY :0.46853515625\n",
      "VALIDATION LOSS :0.8826996479183435\n",
      "VALIDATION ACCURACY :0.258544921875\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3163380231335759\n",
      "TRAINING ACCURACY :0.48849609375\n",
      "VALIDATION LOSS :0.8886660411953926\n",
      "VALIDATION ACCURACY :0.275146484375\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2959058075025678\n",
      "TRAINING ACCURACY :0.512578125\n",
      "VALIDATION LOSS :0.8880087248980999\n",
      "VALIDATION ACCURACY :0.305419921875\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2795801342651248\n",
      "TRAINING ACCURACY :0.53205078125\n",
      "VALIDATION LOSS :0.9334272630512714\n",
      "VALIDATION ACCURACY :0.301513671875\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.26112880479544404\n",
      "TRAINING ACCURACY :0.5530078125\n",
      "VALIDATION LOSS :0.9367818646132946\n",
      "VALIDATION ACCURACY :0.297119140625\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2398567619547248\n",
      "TRAINING ACCURACY :0.58138671875\n",
      "VALIDATION LOSS :0.9409687463194132\n",
      "VALIDATION ACCURACY :0.29443359375\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.22313013594597578\n",
      "TRAINING ACCURACY :0.603515625\n",
      "VALIDATION LOSS :0.9628093130886555\n",
      "VALIDATION ACCURACY :0.314453125\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2083627111837268\n",
      "TRAINING ACCURACY :0.626171875\n",
      "VALIDATION LOSS :1.0043848734349012\n",
      "VALIDATION ACCURACY :0.32568359375\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.18940511036664248\n",
      "TRAINING ACCURACY :0.65275390625\n",
      "VALIDATION LOSS :1.071432987228036\n",
      "VALIDATION ACCURACY :0.312744140625\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.17868304474279284\n",
      "TRAINING ACCURACY :0.670859375\n",
      "VALIDATION LOSS :1.045213920995593\n",
      "VALIDATION ACCURACY :0.325439453125\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.16755628006532788\n",
      "TRAINING ACCURACY :0.689921875\n",
      "VALIDATION LOSS :1.044843677431345\n",
      "VALIDATION ACCURACY :0.326904296875\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.15904195951297878\n",
      "TRAINING ACCURACY :0.70560546875\n",
      "VALIDATION LOSS :1.0752213802188635\n",
      "VALIDATION ACCURACY :0.32177734375\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.14361370753496885\n",
      "TRAINING ACCURACY :0.72974609375\n",
      "VALIDATION LOSS :1.0858600363135338\n",
      "VALIDATION ACCURACY :0.32421875\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.13396955279633402\n",
      "TRAINING ACCURACY :0.74689453125\n",
      "VALIDATION LOSS :1.1594337839633226\n",
      "VALIDATION ACCURACY :0.318359375\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.1286777863278985\n",
      "TRAINING ACCURACY :0.75916015625\n",
      "VALIDATION LOSS :1.1746886502951384\n",
      "VALIDATION ACCURACY :0.316650390625\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.12135204683989287\n",
      "TRAINING ACCURACY :0.773671875\n",
      "VALIDATION LOSS :1.220462353900075\n",
      "VALIDATION ACCURACY :0.3193359375\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.11090077714063228\n",
      "TRAINING ACCURACY :0.7891015625\n",
      "VALIDATION LOSS :1.2230958435684443\n",
      "VALIDATION ACCURACY :0.34423828125\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.10468432023189962\n",
      "TRAINING ACCURACY :0.80185546875\n",
      "VALIDATION LOSS :1.2360671497881413\n",
      "VALIDATION ACCURACY :0.33251953125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e4c4964b05486ba53dfb491fe16a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▂▂▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>training_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█▇████▇▇▇██</td></tr><tr><td>validation_loss</td><td>█▃▂▂▂▁▁▂▁▁▂▂▂▂▂▂▂▂▃▃▄▃▃▄▄▅▅▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>30</td></tr><tr><td>training_accuracy</td><td>0.80186</td></tr><tr><td>training_loss</td><td>0.10468</td></tr><tr><td>validation_accuracy</td><td>0.33252</td></tr><tr><td>validation_loss</td><td>1.23607</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/iku3rg3c' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/iku3rg3c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_162609-iku3rg3c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dvfhtdo8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004885708232072144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_170003-dvfhtdo8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/dvfhtdo8' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/dvfhtdo8' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/dvfhtdo8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.0517615312337876\n",
      "TRAINING ACCURACY :0.01892578125\n",
      "VALIDATION LOSS :1.9935546554625034\n",
      "VALIDATION ACCURACY :0.00146484375\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1861068885028363\n",
      "TRAINING ACCURACY :0.06931640625\n",
      "VALIDATION LOSS :1.6571651957929134\n",
      "VALIDATION ACCURACY :0.01220703125\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9935606755316257\n",
      "TRAINING ACCURACY :0.100703125\n",
      "VALIDATION LOSS :1.2947236485779285\n",
      "VALIDATION ACCURACY :0.052490234375\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8927354267239571\n",
      "TRAINING ACCURACY :0.125625\n",
      "VALIDATION LOSS :1.1111617498099804\n",
      "VALIDATION ACCURACY :0.106201171875\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8238111472129822\n",
      "TRAINING ACCURACY :0.1511328125\n",
      "VALIDATION LOSS :1.1633566468954086\n",
      "VALIDATION ACCURACY :0.088623046875\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7677522832155228\n",
      "TRAINING ACCURACY :0.17365234375\n",
      "VALIDATION LOSS :1.0520382970571518\n",
      "VALIDATION ACCURACY :0.139404296875\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7128145871311427\n",
      "TRAINING ACCURACY :0.1903515625\n",
      "VALIDATION LOSS :1.0092644691467285\n",
      "VALIDATION ACCURACY :0.1455078125\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6893233819305897\n",
      "TRAINING ACCURACY :0.203125\n",
      "VALIDATION LOSS :1.0464916843920946\n",
      "VALIDATION ACCURACY :0.169921875\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6529585082083941\n",
      "TRAINING ACCURACY :0.21880859375\n",
      "VALIDATION LOSS :0.9417141880840063\n",
      "VALIDATION ACCURACY :0.201416015625\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.631836181730032\n",
      "TRAINING ACCURACY :0.2297265625\n",
      "VALIDATION LOSS :1.021650243550539\n",
      "VALIDATION ACCURACY :0.150390625\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6257586781680584\n",
      "TRAINING ACCURACY :0.23603515625\n",
      "VALIDATION LOSS :1.0156305339187384\n",
      "VALIDATION ACCURACY :0.162109375\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5912655175477266\n",
      "TRAINING ACCURACY :0.25021484375\n",
      "VALIDATION LOSS :1.1593376137316227\n",
      "VALIDATION ACCURACY :0.1318359375\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5798105848580599\n",
      "TRAINING ACCURACY :0.254765625\n",
      "VALIDATION LOSS :0.9341490212827921\n",
      "VALIDATION ACCURACY :0.216796875\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5706606002897024\n",
      "TRAINING ACCURACY :0.26328125\n",
      "VALIDATION LOSS :0.9590183161199093\n",
      "VALIDATION ACCURACY :0.20263671875\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5569849213957787\n",
      "TRAINING ACCURACY :0.27150390625\n",
      "VALIDATION LOSS :0.9092529658228159\n",
      "VALIDATION ACCURACY :0.22265625\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5480021431297064\n",
      "TRAINING ACCURACY :0.27705078125\n",
      "VALIDATION LOSS :0.9131389409303665\n",
      "VALIDATION ACCURACY :0.22412109375\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5347094088792801\n",
      "TRAINING ACCURACY :0.28220703125\n",
      "VALIDATION LOSS :0.9233987238258123\n",
      "VALIDATION ACCURACY :0.215087890625\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5258374466001987\n",
      "TRAINING ACCURACY :0.2886328125\n",
      "VALIDATION LOSS :0.9268637597560883\n",
      "VALIDATION ACCURACY :0.23291015625\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5221655680984258\n",
      "TRAINING ACCURACY :0.29283203125\n",
      "VALIDATION LOSS :0.9276752881705761\n",
      "VALIDATION ACCURACY :0.208251953125\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5078435602784157\n",
      "TRAINING ACCURACY :0.30466796875\n",
      "VALIDATION LOSS :0.8738147467374802\n",
      "VALIDATION ACCURACY :0.25439453125\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4969717874377966\n",
      "TRAINING ACCURACY :0.30947265625\n",
      "VALIDATION LOSS :0.9347726032137871\n",
      "VALIDATION ACCURACY :0.20458984375\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4904042450338602\n",
      "TRAINING ACCURACY :0.31517578125\n",
      "VALIDATION LOSS :0.8707612194120884\n",
      "VALIDATION ACCURACY :0.254150390625\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.48395391143858435\n",
      "TRAINING ACCURACY :0.318984375\n",
      "VALIDATION LOSS :0.8809686042368412\n",
      "VALIDATION ACCURACY :0.2373046875\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4767477080225945\n",
      "TRAINING ACCURACY :0.32587890625\n",
      "VALIDATION LOSS :0.8772084061056376\n",
      "VALIDATION ACCURACY :0.2509765625\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.46637543782591817\n",
      "TRAINING ACCURACY :0.3315625\n",
      "VALIDATION LOSS :0.9077795874327421\n",
      "VALIDATION ACCURACY :0.23046875\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.45350622937083246\n",
      "TRAINING ACCURACY :0.3410546875\n",
      "VALIDATION LOSS :0.858728839084506\n",
      "VALIDATION ACCURACY :0.26220703125\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.45529170148074627\n",
      "TRAINING ACCURACY :0.33921875\n",
      "VALIDATION LOSS :0.8355450127273798\n",
      "VALIDATION ACCURACY :0.27099609375\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4538586626201868\n",
      "TRAINING ACCURACY :0.34431640625\n",
      "VALIDATION LOSS :0.8521657232195139\n",
      "VALIDATION ACCURACY :0.293701171875\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4433999850600958\n",
      "TRAINING ACCURACY :0.35060546875\n",
      "VALIDATION LOSS :0.8537006862461567\n",
      "VALIDATION ACCURACY :0.27392578125\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4386679804325104\n",
      "TRAINING ACCURACY :0.355\n",
      "VALIDATION LOSS :0.8552791401743889\n",
      "VALIDATION ACCURACY :0.279541015625\n",
      "\n",
      " EPOCH: 31\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.43320401549339294\n",
      "TRAINING ACCURACY :0.35818359375\n",
      "VALIDATION LOSS :0.8763701915740967\n",
      "VALIDATION ACCURACY :0.28125\n",
      "\n",
      " EPOCH: 32\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.42975653424859045\n",
      "TRAINING ACCURACY :0.3628515625\n",
      "VALIDATION LOSS :0.8737185690551996\n",
      "VALIDATION ACCURACY :0.2724609375\n",
      "\n",
      " EPOCH: 33\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4199323368817568\n",
      "TRAINING ACCURACY :0.3700390625\n",
      "VALIDATION LOSS :0.8589880503714085\n",
      "VALIDATION ACCURACY :0.27685546875\n",
      "\n",
      " EPOCH: 34\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.41030156597495077\n",
      "TRAINING ACCURACY :0.3761328125\n",
      "VALIDATION LOSS :0.8538139816373587\n",
      "VALIDATION ACCURACY :0.293701171875\n",
      "\n",
      " EPOCH: 35\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4053727241232991\n",
      "TRAINING ACCURACY :0.3770703125\n",
      "VALIDATION LOSS :0.8503769803792238\n",
      "VALIDATION ACCURACY :0.2890625\n",
      "\n",
      " EPOCH: 36\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4053367181867361\n",
      "TRAINING ACCURACY :0.38205078125\n",
      "VALIDATION LOSS :0.8764362204819918\n",
      "VALIDATION ACCURACY :0.267578125\n",
      "\n",
      " EPOCH: 37\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4077731515467167\n",
      "TRAINING ACCURACY :0.38478515625\n",
      "VALIDATION LOSS :0.9150319714099169\n",
      "VALIDATION ACCURACY :0.256591796875\n",
      "\n",
      " EPOCH: 38\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.40041578471660616\n",
      "TRAINING ACCURACY :0.39119140625\n",
      "VALIDATION LOSS :0.9219822231680155\n",
      "VALIDATION ACCURACY :0.25048828125\n",
      "\n",
      " EPOCH: 39\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3863394171744585\n",
      "TRAINING ACCURACY :0.39365234375\n",
      "VALIDATION LOSS :0.8701075110584497\n",
      "VALIDATION ACCURACY :0.29150390625\n",
      "\n",
      " EPOCH: 40\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3964781020209193\n",
      "TRAINING ACCURACY :0.393828125\n",
      "VALIDATION LOSS :0.9003547765314579\n",
      "VALIDATION ACCURACY :0.27490234375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced0ba911ced4389b1b1ee46dec609d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▂▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>training_loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▂▄▃▄▄▅▆▅▅▄▆▆▆▆▆▇▆▇▆▇▇▇▆▇▇████▇███▇▇▇██</td></tr><tr><td>validation_loss</td><td>█▆▄▃▃▂▂▂▂▂▂▃▂▂▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>40</td></tr><tr><td>training_accuracy</td><td>0.39383</td></tr><tr><td>training_loss</td><td>0.39648</td></tr><tr><td>validation_accuracy</td><td>0.2749</td></tr><tr><td>validation_loss</td><td>0.90035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/dvfhtdo8' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/dvfhtdo8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_170003-dvfhtdo8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hq491a36 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005132905090003188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_170700-hq491a36</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/hq491a36' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/hq491a36' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/hq491a36</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.6354526763409376\n",
      "TRAINING ACCURACY :0.044609375\n",
      "VALIDATION LOSS :1.469151696190238\n",
      "VALIDATION ACCURACY :0.005859375\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9423057006299496\n",
      "TRAINING ACCURACY :0.128984375\n",
      "VALIDATION LOSS :1.10893424320966\n",
      "VALIDATION ACCURACY :0.0673828125\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7822970907762646\n",
      "TRAINING ACCURACY :0.18220703125\n",
      "VALIDATION LOSS :1.205957479774952\n",
      "VALIDATION ACCURACY :0.0615234375\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6898278763145208\n",
      "TRAINING ACCURACY :0.219921875\n",
      "VALIDATION LOSS :0.9110129913315177\n",
      "VALIDATION ACCURACY :0.1591796875\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.630383296906948\n",
      "TRAINING ACCURACY :0.24625\n",
      "VALIDATION LOSS :1.059973431751132\n",
      "VALIDATION ACCURACY :0.134033203125\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5899958190321922\n",
      "TRAINING ACCURACY :0.26955078125\n",
      "VALIDATION LOSS :0.8669262891635299\n",
      "VALIDATION ACCURACY :0.2353515625\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5589094207063318\n",
      "TRAINING ACCURACY :0.285625\n",
      "VALIDATION LOSS :0.8940231204032898\n",
      "VALIDATION ACCURACY :0.20751953125\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5283417667448521\n",
      "TRAINING ACCURACY :0.30640625\n",
      "VALIDATION LOSS :0.8520415555685759\n",
      "VALIDATION ACCURACY :0.23291015625\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5112757415696979\n",
      "TRAINING ACCURACY :0.32150390625\n",
      "VALIDATION LOSS :0.874985390342772\n",
      "VALIDATION ACCURACY :0.243408203125\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.48547902181744573\n",
      "TRAINING ACCURACY :0.33861328125\n",
      "VALIDATION LOSS :0.8387344423681498\n",
      "VALIDATION ACCURACY :0.272705078125\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4679839450120926\n",
      "TRAINING ACCURACY :0.351875\n",
      "VALIDATION LOSS :0.8986190790310502\n",
      "VALIDATION ACCURACY :0.240966796875\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.44393289513885975\n",
      "TRAINING ACCURACY :0.36857421875\n",
      "VALIDATION LOSS :0.900945614092052\n",
      "VALIDATION ACCURACY :0.250244140625\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4373925312049687\n",
      "TRAINING ACCURACY :0.3745703125\n",
      "VALIDATION LOSS :0.8995466316118836\n",
      "VALIDATION ACCURACY :0.2734375\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4263465337269008\n",
      "TRAINING ACCURACY :0.38599609375\n",
      "VALIDATION LOSS :0.8664408531039953\n",
      "VALIDATION ACCURACY :0.314697265625\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.40623136688023803\n",
      "TRAINING ACCURACY :0.401640625\n",
      "VALIDATION LOSS :0.8637680439278483\n",
      "VALIDATION ACCURACY :0.29833984375\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.39519863344728945\n",
      "TRAINING ACCURACY :0.40783203125\n",
      "VALIDATION LOSS :0.9328706627711654\n",
      "VALIDATION ACCURACY :0.258544921875\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3791045031324029\n",
      "TRAINING ACCURACY :0.42556640625\n",
      "VALIDATION LOSS :0.8577489284798503\n",
      "VALIDATION ACCURACY :0.3046875\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.36759519185870887\n",
      "TRAINING ACCURACY :0.4369921875\n",
      "VALIDATION LOSS :0.8836984904482961\n",
      "VALIDATION ACCURACY :0.301025390625\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.35788431657478215\n",
      "TRAINING ACCURACY :0.4451171875\n",
      "VALIDATION LOSS :0.9398344503715634\n",
      "VALIDATION ACCURACY :0.25634765625\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3467868653126061\n",
      "TRAINING ACCURACY :0.4584375\n",
      "VALIDATION LOSS :0.9411241542547941\n",
      "VALIDATION ACCURACY :0.28955078125\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.33975627249106766\n",
      "TRAINING ACCURACY :0.46640625\n",
      "VALIDATION LOSS :0.9476108057424426\n",
      "VALIDATION ACCURACY :0.296630859375\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.331998401787132\n",
      "TRAINING ACCURACY :0.47310546875\n",
      "VALIDATION LOSS :0.9203302450478077\n",
      "VALIDATION ACCURACY :0.31787109375\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.32504508486017586\n",
      "TRAINING ACCURACY :0.48392578125\n",
      "VALIDATION LOSS :0.9906327920034528\n",
      "VALIDATION ACCURACY :0.2998046875\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3149383296072483\n",
      "TRAINING ACCURACY :0.4922265625\n",
      "VALIDATION LOSS :0.9876835942268372\n",
      "VALIDATION ACCURACY :0.299560546875\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3079028248228133\n",
      "TRAINING ACCURACY :0.5022265625\n",
      "VALIDATION LOSS :1.0005340771749616\n",
      "VALIDATION ACCURACY :0.31396484375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b14aa0c795464d92c341cf108692da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>training_accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>training_loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▂▄▄▆▆▆▆▇▆▆▇██▇██▇▇█████</td></tr><tr><td>validation_loss</td><td>█▄▅▂▃▁▂▁▁▁▂▂▂▁▁▂▁▁▂▂▂▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>25</td></tr><tr><td>training_accuracy</td><td>0.50223</td></tr><tr><td>training_loss</td><td>0.3079</td></tr><tr><td>validation_accuracy</td><td>0.31396</td></tr><tr><td>validation_loss</td><td>1.00053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/hq491a36' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/hq491a36</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_170700-hq491a36/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: czz27iex with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005397779295245354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RAdam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_171634-czz27iex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/czz27iex' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/czz27iex' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/czz27iex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.4238476176559924\n",
      "TRAINING ACCURACY :0.00697265625\n",
      "VALIDATION LOSS :1.3617533948272467\n",
      "VALIDATION ACCURACY :0.03857421875\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1732499679923059\n",
      "TRAINING ACCURACY :0.07384765625\n",
      "VALIDATION LOSS :1.10653021838516\n",
      "VALIDATION ACCURACY :0.08984375\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.912893572896719\n",
      "TRAINING ACCURACY :0.135625\n",
      "VALIDATION LOSS :1.0244549103081226\n",
      "VALIDATION ACCURACY :0.119384765625\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7775288724899292\n",
      "TRAINING ACCURACY :0.18376953125\n",
      "VALIDATION LOSS :1.0017430316656828\n",
      "VALIDATION ACCURACY :0.13525390625\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6917737421765924\n",
      "TRAINING ACCURACY :0.21427734375\n",
      "VALIDATION LOSS :0.9654690669849515\n",
      "VALIDATION ACCURACY :0.141357421875\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6270981406420469\n",
      "TRAINING ACCURACY :0.24666015625\n",
      "VALIDATION LOSS :0.8343316726386547\n",
      "VALIDATION ACCURACY :0.25\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5865005546808243\n",
      "TRAINING ACCURACY :0.2707421875\n",
      "VALIDATION LOSS :0.8422077484428883\n",
      "VALIDATION ACCURACY :0.25\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5592256564646959\n",
      "TRAINING ACCURACY :0.287890625\n",
      "VALIDATION LOSS :0.8316969461739063\n",
      "VALIDATION ACCURACY :0.2490234375\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5301499947160483\n",
      "TRAINING ACCURACY :0.30603515625\n",
      "VALIDATION LOSS :0.8591945972293615\n",
      "VALIDATION ACCURACY :0.23876953125\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5015630747750401\n",
      "TRAINING ACCURACY :0.32287109375\n",
      "VALIDATION LOSS :0.8335751574486494\n",
      "VALIDATION ACCURACY :0.27587890625\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.47778700970113275\n",
      "TRAINING ACCURACY :0.34076171875\n",
      "VALIDATION LOSS :0.8328876690939069\n",
      "VALIDATION ACCURACY :0.2724609375\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.45342972511425617\n",
      "TRAINING ACCURACY :0.35921875\n",
      "VALIDATION LOSS :0.808328595943749\n",
      "VALIDATION ACCURACY :0.288818359375\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.43979128170758486\n",
      "TRAINING ACCURACY :0.3712109375\n",
      "VALIDATION LOSS :0.8068419490009546\n",
      "VALIDATION ACCURACY :0.29052734375\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4170854526758194\n",
      "TRAINING ACCURACY :0.38826171875\n",
      "VALIDATION LOSS :0.8045989479869604\n",
      "VALIDATION ACCURACY :0.298095703125\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4015792197547853\n",
      "TRAINING ACCURACY :0.40517578125\n",
      "VALIDATION LOSS :0.7942072581499815\n",
      "VALIDATION ACCURACY :0.327392578125\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.38678348625078796\n",
      "TRAINING ACCURACY :0.4178515625\n",
      "VALIDATION LOSS :0.8232358926907182\n",
      "VALIDATION ACCURACY :0.304443359375\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3742064771428704\n",
      "TRAINING ACCURACY :0.4321484375\n",
      "VALIDATION LOSS :0.8195469314232469\n",
      "VALIDATION ACCURACY :0.32177734375\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.36024586133658887\n",
      "TRAINING ACCURACY :0.443515625\n",
      "VALIDATION LOSS :0.8208479909226298\n",
      "VALIDATION ACCURACY :0.335693359375\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.34334015764296055\n",
      "TRAINING ACCURACY :0.4540234375\n",
      "VALIDATION LOSS :0.8371886862441897\n",
      "VALIDATION ACCURACY :0.3212890625\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.33301303308457136\n",
      "TRAINING ACCURACY :0.46818359375\n",
      "VALIDATION LOSS :0.83719826862216\n",
      "VALIDATION ACCURACY :0.342529296875\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3223854416795075\n",
      "TRAINING ACCURACY :0.48107421875\n",
      "VALIDATION LOSS :0.8440302144736052\n",
      "VALIDATION ACCURACY :0.334716796875\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.31080513332039117\n",
      "TRAINING ACCURACY :0.49314453125\n",
      "VALIDATION LOSS :0.8579626372084022\n",
      "VALIDATION ACCURACY :0.332275390625\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2960285340808332\n",
      "TRAINING ACCURACY :0.50759765625\n",
      "VALIDATION LOSS :0.884228196926415\n",
      "VALIDATION ACCURACY :0.34814453125\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2872938947193325\n",
      "TRAINING ACCURACY :0.5198828125\n",
      "VALIDATION LOSS :0.8759170118719339\n",
      "VALIDATION ACCURACY :0.342041015625\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2737786122597754\n",
      "TRAINING ACCURACY :0.53275390625\n",
      "VALIDATION LOSS :0.9114884315058589\n",
      "VALIDATION ACCURACY :0.33740234375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ee1942eb5042d69a4ebd5a1b948bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>training_accuracy</td><td>▁▂▃▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▃▃▃▆▆▆▆▆▆▇▇▇█▇▇█▇██████</td></tr><tr><td>validation_loss</td><td>█▅▄▄▃▁▂▁▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>25</td></tr><tr><td>training_accuracy</td><td>0.53275</td></tr><tr><td>training_loss</td><td>0.27378</td></tr><tr><td>validation_accuracy</td><td>0.3374</td></tr><tr><td>validation_loss</td><td>0.91149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/czz27iex' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/czz27iex</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_171634-czz27iex/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qcogci0k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00011855996115818143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_172739-qcogci0k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/qcogci0k' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/qcogci0k' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/qcogci0k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :3.498865964412689\n",
      "TRAINING ACCURACY :0.0\n",
      "VALIDATION LOSS :3.1414350867271423\n",
      "VALIDATION ACCURACY :0.0\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.9189817690849305\n",
      "TRAINING ACCURACY :3.90625e-05\n",
      "VALIDATION LOSS :2.723590597510338\n",
      "VALIDATION ACCURACY :0.00048828125\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.5526170551776888\n",
      "TRAINING ACCURACY :0.00013671875\n",
      "VALIDATION LOSS :2.3603220880031586\n",
      "VALIDATION ACCURACY :0.000244140625\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.240159876346588\n",
      "TRAINING ACCURACY :0.000703125\n",
      "VALIDATION LOSS :2.1391284316778183\n",
      "VALIDATION ACCURACY :0.002197265625\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.9860744327306747\n",
      "TRAINING ACCURACY :0.003359375\n",
      "VALIDATION LOSS :1.9183531999588013\n",
      "VALIDATION ACCURACY :0.005615234375\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.7726154160499572\n",
      "TRAINING ACCURACY :0.01013671875\n",
      "VALIDATION LOSS :1.7368781045079231\n",
      "VALIDATION ACCURACY :0.0087890625\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.5931007468700409\n",
      "TRAINING ACCURACY :0.0198828125\n",
      "VALIDATION LOSS :1.6879742443561554\n",
      "VALIDATION ACCURACY :0.00439453125\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.4685480624437333\n",
      "TRAINING ACCURACY :0.02947265625\n",
      "VALIDATION LOSS :1.4863958433270454\n",
      "VALIDATION ACCURACY :0.015869140625\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.3659732526540755\n",
      "TRAINING ACCURACY :0.03931640625\n",
      "VALIDATION LOSS :1.4801890701055527\n",
      "VALIDATION ACCURACY :0.016357421875\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.2900142151117324\n",
      "TRAINING ACCURACY :0.05046875\n",
      "VALIDATION LOSS :1.3645483702421188\n",
      "VALIDATION ACCURACY :0.024169921875\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.2234276580810546\n",
      "TRAINING ACCURACY :0.05876953125\n",
      "VALIDATION LOSS :1.3530055806040764\n",
      "VALIDATION ACCURACY :0.020751953125\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1789619451761246\n",
      "TRAINING ACCURACY :0.06466796875\n",
      "VALIDATION LOSS :1.3160585090517998\n",
      "VALIDATION ACCURACY :0.025634765625\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1272236147522927\n",
      "TRAINING ACCURACY :0.07697265625\n",
      "VALIDATION LOSS :1.2888609021902084\n",
      "VALIDATION ACCURACY :0.02490234375\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.0951633158326148\n",
      "TRAINING ACCURACY :0.08005859375\n",
      "VALIDATION LOSS :1.190165065228939\n",
      "VALIDATION ACCURACY :0.050048828125\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.0683600190281868\n",
      "TRAINING ACCURACY :0.08744140625\n",
      "VALIDATION LOSS :1.29962807148695\n",
      "VALIDATION ACCURACY :0.02001953125\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.0391841933131218\n",
      "TRAINING ACCURACY :0.0965234375\n",
      "VALIDATION LOSS :1.2739309817552567\n",
      "VALIDATION ACCURACY :0.031494140625\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.014909968674183\n",
      "TRAINING ACCURACY :0.10080078125\n",
      "VALIDATION LOSS :1.1906376332044601\n",
      "VALIDATION ACCURACY :0.04931640625\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9846115726232528\n",
      "TRAINING ACCURACY :0.1101171875\n",
      "VALIDATION LOSS :1.3047309890389442\n",
      "VALIDATION ACCURACY :0.028076171875\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9533519521355629\n",
      "TRAINING ACCURACY :0.1184375\n",
      "VALIDATION LOSS :1.173518031835556\n",
      "VALIDATION ACCURACY :0.043701171875\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9376696878671646\n",
      "TRAINING ACCURACY :0.12494140625\n",
      "VALIDATION LOSS :1.1614912301301956\n",
      "VALIDATION ACCURACY :0.05419921875\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9159747985005379\n",
      "TRAINING ACCURACY :0.13125\n",
      "VALIDATION LOSS :1.1029741913080215\n",
      "VALIDATION ACCURACY :0.064208984375\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8976312726736069\n",
      "TRAINING ACCURACY :0.13802734375\n",
      "VALIDATION LOSS :1.1171272806823254\n",
      "VALIDATION ACCURACY :0.0703125\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8807310703396797\n",
      "TRAINING ACCURACY :0.14009765625\n",
      "VALIDATION LOSS :1.1761775314807892\n",
      "VALIDATION ACCURACY :0.055908203125\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8649623322486878\n",
      "TRAINING ACCURACY :0.14703125\n",
      "VALIDATION LOSS :1.2201998382806778\n",
      "VALIDATION ACCURACY :0.044921875\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8489330470561981\n",
      "TRAINING ACCURACY :0.15400390625\n",
      "VALIDATION LOSS :1.080696426331997\n",
      "VALIDATION ACCURACY :0.07470703125\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8357085058093071\n",
      "TRAINING ACCURACY :0.15771484375\n",
      "VALIDATION LOSS :1.1750254929065704\n",
      "VALIDATION ACCURACY :0.054931640625\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8182195201516151\n",
      "TRAINING ACCURACY :0.1651171875\n",
      "VALIDATION LOSS :1.1485774517059326\n",
      "VALIDATION ACCURACY :0.060791015625\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8004657566547394\n",
      "TRAINING ACCURACY :0.1708203125\n",
      "VALIDATION LOSS :1.1017575114965439\n",
      "VALIDATION ACCURACY :0.083740234375\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7883598500490189\n",
      "TRAINING ACCURACY :0.1756640625\n",
      "VALIDATION LOSS :1.1638174280524254\n",
      "VALIDATION ACCURACY :0.059326171875\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7833678346872329\n",
      "TRAINING ACCURACY :0.1769140625\n",
      "VALIDATION LOSS :1.0617473274469376\n",
      "VALIDATION ACCURACY :0.099609375\n",
      "\n",
      " EPOCH: 31\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.775588458776474\n",
      "TRAINING ACCURACY :0.1796484375\n",
      "VALIDATION LOSS :1.0882785990834236\n",
      "VALIDATION ACCURACY :0.087158203125\n",
      "\n",
      " EPOCH: 32\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7643159627914429\n",
      "TRAINING ACCURACY :0.1840234375\n",
      "VALIDATION LOSS :1.0500052012503147\n",
      "VALIDATION ACCURACY :0.10986328125\n",
      "\n",
      " EPOCH: 33\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7529533997178077\n",
      "TRAINING ACCURACY :0.1912109375\n",
      "VALIDATION LOSS :1.0433694124221802\n",
      "VALIDATION ACCURACY :0.121826171875\n",
      "\n",
      " EPOCH: 34\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7439045435190201\n",
      "TRAINING ACCURACY :0.19275390625\n",
      "VALIDATION LOSS :1.104795541614294\n",
      "VALIDATION ACCURACY :0.080078125\n",
      "\n",
      " EPOCH: 35\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7277978587150574\n",
      "TRAINING ACCURACY :0.19982421875\n",
      "VALIDATION LOSS :1.0843815244734287\n",
      "VALIDATION ACCURACY :0.102783203125\n",
      "\n",
      " EPOCH: 36\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7140742978453636\n",
      "TRAINING ACCURACY :0.2058984375\n",
      "VALIDATION LOSS :0.9498674273490906\n",
      "VALIDATION ACCURACY :0.158203125\n",
      "\n",
      " EPOCH: 37\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7190321460366249\n",
      "TRAINING ACCURACY :0.2056640625\n",
      "VALIDATION LOSS :1.0661205500364304\n",
      "VALIDATION ACCURACY :0.103515625\n",
      "\n",
      " EPOCH: 38\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7120752713084221\n",
      "TRAINING ACCURACY :0.20845703125\n",
      "VALIDATION LOSS :1.097879871726036\n",
      "VALIDATION ACCURACY :0.08837890625\n",
      "\n",
      " EPOCH: 39\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6997794367372989\n",
      "TRAINING ACCURACY :0.2119140625\n",
      "VALIDATION LOSS :0.986589603126049\n",
      "VALIDATION ACCURACY :0.129150390625\n",
      "\n",
      " EPOCH: 40\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6889432287216186\n",
      "TRAINING ACCURACY :0.216171875\n",
      "VALIDATION LOSS :0.9736808463931084\n",
      "VALIDATION ACCURACY :0.145751953125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a68113f1fe4669961a42b2704c0502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>training_loss</td><td>█▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▃▂▂▃▂▃▃▄▄▃▃▄▃▄▅▄▅▅▆▆▅▆█▆▅▇▇</td></tr><tr><td>validation_loss</td><td>█▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>40</td></tr><tr><td>training_accuracy</td><td>0.21617</td></tr><tr><td>training_loss</td><td>0.68894</td></tr><tr><td>validation_accuracy</td><td>0.14575</td></tr><tr><td>validation_loss</td><td>0.97368</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/qcogci0k' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/qcogci0k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_172739-qcogci0k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kbcfyknm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004432240510577218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_173337-kbcfyknm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/kbcfyknm' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/kbcfyknm' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/kbcfyknm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.8499194526672362\n",
      "TRAINING ACCURACY :0.02501953125\n",
      "VALIDATION LOSS :1.4707735255360603\n",
      "VALIDATION ACCURACY :0.007080078125\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1383085753023625\n",
      "TRAINING ACCURACY :0.07794921875\n",
      "VALIDATION LOSS :1.3124392442405224\n",
      "VALIDATION ACCURACY :0.01513671875\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9545404380559921\n",
      "TRAINING ACCURACY :0.12232421875\n",
      "VALIDATION LOSS :1.0693686325103045\n",
      "VALIDATION ACCURACY :0.091796875\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8726873156428337\n",
      "TRAINING ACCURACY :0.146875\n",
      "VALIDATION LOSS :1.0840191300958395\n",
      "VALIDATION ACCURACY :0.1142578125\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7904558262228966\n",
      "TRAINING ACCURACY :0.17671875\n",
      "VALIDATION LOSS :1.099374933168292\n",
      "VALIDATION ACCURACY :0.100341796875\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.747814016342163\n",
      "TRAINING ACCURACY :0.19373046875\n",
      "VALIDATION LOSS :1.137784417718649\n",
      "VALIDATION ACCURACY :0.110595703125\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6981394796818495\n",
      "TRAINING ACCURACY :0.21259765625\n",
      "VALIDATION LOSS :1.0925343558192253\n",
      "VALIDATION ACCURACY :0.123046875\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6801147521287203\n",
      "TRAINING ACCURACY :0.2246875\n",
      "VALIDATION LOSS :1.1617569848895073\n",
      "VALIDATION ACCURACY :0.102783203125\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6588914278149605\n",
      "TRAINING ACCURACY :0.23546875\n",
      "VALIDATION LOSS :0.9597798548638821\n",
      "VALIDATION ACCURACY :0.161865234375\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6390810380131007\n",
      "TRAINING ACCURACY :0.2457421875\n",
      "VALIDATION LOSS :0.9778387825936079\n",
      "VALIDATION ACCURACY :0.18505859375\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6109337538480759\n",
      "TRAINING ACCURACY :0.259375\n",
      "VALIDATION LOSS :0.9551767949014902\n",
      "VALIDATION ACCURACY :0.193115234375\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5986754892766476\n",
      "TRAINING ACCURACY :0.266015625\n",
      "VALIDATION LOSS :0.8755857888609171\n",
      "VALIDATION ACCURACY :0.215576171875\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5772061516344548\n",
      "TRAINING ACCURACY :0.2757421875\n",
      "VALIDATION LOSS :0.9080674834549427\n",
      "VALIDATION ACCURACY :0.2099609375\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5590862304717302\n",
      "TRAINING ACCURACY :0.285703125\n",
      "VALIDATION LOSS :0.9281800128519535\n",
      "VALIDATION ACCURACY :0.195556640625\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5557568596303463\n",
      "TRAINING ACCURACY :0.29142578125\n",
      "VALIDATION LOSS :0.8553463369607925\n",
      "VALIDATION ACCURACY :0.2646484375\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5479731577634811\n",
      "TRAINING ACCURACY :0.29919921875\n",
      "VALIDATION LOSS :0.847072871401906\n",
      "VALIDATION ACCURACY :0.245361328125\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5350564884394408\n",
      "TRAINING ACCURACY :0.305546875\n",
      "VALIDATION LOSS :0.9107839725911617\n",
      "VALIDATION ACCURACY :0.2509765625\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5267090238630772\n",
      "TRAINING ACCURACY :0.3102734375\n",
      "VALIDATION LOSS :0.8838712275028229\n",
      "VALIDATION ACCURACY :0.246337890625\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5128908771276474\n",
      "TRAINING ACCURACY :0.31986328125\n",
      "VALIDATION LOSS :0.8755023516714573\n",
      "VALIDATION ACCURACY :0.23486328125\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4979677081853151\n",
      "TRAINING ACCURACY :0.3273046875\n",
      "VALIDATION LOSS :0.8306305184960365\n",
      "VALIDATION ACCURACY :0.27734375\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.49008209399878977\n",
      "TRAINING ACCURACY :0.3355078125\n",
      "VALIDATION LOSS :0.8887851778417826\n",
      "VALIDATION ACCURACY :0.251220703125\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4822847534716129\n",
      "TRAINING ACCURACY :0.340078125\n",
      "VALIDATION LOSS :0.8685599807649851\n",
      "VALIDATION ACCURACY :0.251220703125\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4803890755772591\n",
      "TRAINING ACCURACY :0.343203125\n",
      "VALIDATION LOSS :0.8504833970218897\n",
      "VALIDATION ACCURACY :0.282470703125\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.47643476895987985\n",
      "TRAINING ACCURACY :0.34849609375\n",
      "VALIDATION LOSS :0.8557529449462891\n",
      "VALIDATION ACCURACY :0.27734375\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4620969315618277\n",
      "TRAINING ACCURACY :0.35759765625\n",
      "VALIDATION LOSS :0.8350571896880865\n",
      "VALIDATION ACCURACY :0.292236328125\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4622580723464489\n",
      "TRAINING ACCURACY :0.35908203125\n",
      "VALIDATION LOSS :0.8517215363681316\n",
      "VALIDATION ACCURACY :0.29345703125\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4497935142368078\n",
      "TRAINING ACCURACY :0.367265625\n",
      "VALIDATION LOSS :0.840978791937232\n",
      "VALIDATION ACCURACY :0.29248046875\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.44879798375070096\n",
      "TRAINING ACCURACY :0.37044921875\n",
      "VALIDATION LOSS :0.8856717478483915\n",
      "VALIDATION ACCURACY :0.290771484375\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.436098482683301\n",
      "TRAINING ACCURACY :0.37828125\n",
      "VALIDATION LOSS :0.8630753699690104\n",
      "VALIDATION ACCURACY :0.29736328125\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4309076606482267\n",
      "TRAINING ACCURACY :0.3825390625\n",
      "VALIDATION LOSS :0.9033178295940161\n",
      "VALIDATION ACCURACY :0.291259765625\n",
      "\n",
      " EPOCH: 31\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4248408868908882\n",
      "TRAINING ACCURACY :0.3905078125\n",
      "VALIDATION LOSS :0.8717371057718992\n",
      "VALIDATION ACCURACY :0.300537109375\n",
      "\n",
      " EPOCH: 32\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4228941647708416\n",
      "TRAINING ACCURACY :0.39337890625\n",
      "VALIDATION LOSS :0.901651419699192\n",
      "VALIDATION ACCURACY :0.281494140625\n",
      "\n",
      " EPOCH: 33\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.41417740724980834\n",
      "TRAINING ACCURACY :0.39939453125\n",
      "VALIDATION LOSS :0.8630758747458458\n",
      "VALIDATION ACCURACY :0.3154296875\n",
      "\n",
      " EPOCH: 34\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.40663503982126714\n",
      "TRAINING ACCURACY :0.401875\n",
      "VALIDATION LOSS :0.90241888910532\n",
      "VALIDATION ACCURACY :0.302978515625\n",
      "\n",
      " EPOCH: 35\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.40555924296379087\n",
      "TRAINING ACCURACY :0.40560546875\n",
      "VALIDATION LOSS :0.8879120741039515\n",
      "VALIDATION ACCURACY :0.32763671875\n",
      "\n",
      " EPOCH: 36\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.39812946528196336\n",
      "TRAINING ACCURACY :0.41130859375\n",
      "VALIDATION LOSS :0.8776131011545658\n",
      "VALIDATION ACCURACY :0.30224609375\n",
      "\n",
      " EPOCH: 37\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.38904470816254616\n",
      "TRAINING ACCURACY :0.4172265625\n",
      "VALIDATION LOSS :0.8761397823691368\n",
      "VALIDATION ACCURACY :0.312255859375\n",
      "\n",
      " EPOCH: 38\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3864449454471469\n",
      "TRAINING ACCURACY :0.42158203125\n",
      "VALIDATION LOSS :0.9018757212907076\n",
      "VALIDATION ACCURACY :0.3310546875\n",
      "\n",
      " EPOCH: 39\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.38385136649012563\n",
      "TRAINING ACCURACY :0.42787109375\n",
      "VALIDATION LOSS :0.9261121638119221\n",
      "VALIDATION ACCURACY :0.295654296875\n",
      "\n",
      " EPOCH: 40\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3712890847027302\n",
      "TRAINING ACCURACY :0.43572265625\n",
      "VALIDATION LOSS :0.949121406301856\n",
      "VALIDATION ACCURACY :0.293212890625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f348396127ee478e9650ea39799ded17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>training_loss</td><td>█▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▃▃▃▃▄▃▄▅▅▆▅▅▇▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇██▇▇</td></tr><tr><td>validation_loss</td><td>█▆▄▄▄▄▄▅▂▃▂▁▂▂▁▁▂▂▁▁▂▁▁▁▁▁▁▂▁▂▁▂▁▂▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>40</td></tr><tr><td>training_accuracy</td><td>0.43572</td></tr><tr><td>training_loss</td><td>0.37129</td></tr><tr><td>validation_accuracy</td><td>0.29321</td></tr><tr><td>validation_loss</td><td>0.94912</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/kbcfyknm' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/kbcfyknm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_173337-kbcfyknm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u5qquafi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001352414943957306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RAdam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_174344-u5qquafi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/u5qquafi' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/u5qquafi' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/u5qquafi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :2.8905639435350894\n",
      "TRAINING ACCURACY :0.002265625\n",
      "VALIDATION LOSS :1.9577084016054869\n",
      "VALIDATION ACCURACY :0.003173828125\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.5075893422961235\n",
      "TRAINING ACCURACY :0.035859375\n",
      "VALIDATION LOSS :1.666482798755169\n",
      "VALIDATION ACCURACY :0.003662109375\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.2067807547748088\n",
      "TRAINING ACCURACY :0.07134765625\n",
      "VALIDATION LOSS :1.3256480526179075\n",
      "VALIDATION ACCURACY :0.033203125\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.050102429986\n",
      "TRAINING ACCURACY :0.099453125\n",
      "VALIDATION LOSS :1.209035056643188\n",
      "VALIDATION ACCURACY :0.068115234375\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.953615794852376\n",
      "TRAINING ACCURACY :0.11884765625\n",
      "VALIDATION LOSS :1.3360782265663147\n",
      "VALIDATION ACCURACY :0.05029296875\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8879408027231693\n",
      "TRAINING ACCURACY :0.1398046875\n",
      "VALIDATION LOSS :1.065110414288938\n",
      "VALIDATION ACCURACY :0.1435546875\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8220840908586979\n",
      "TRAINING ACCURACY :0.161171875\n",
      "VALIDATION LOSS :1.0595682812854648\n",
      "VALIDATION ACCURACY :0.11279296875\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7644546248018742\n",
      "TRAINING ACCURACY :0.18294921875\n",
      "VALIDATION LOSS :1.0565841514617205\n",
      "VALIDATION ACCURACY :0.125732421875\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.723015779927373\n",
      "TRAINING ACCURACY :0.19830078125\n",
      "VALIDATION LOSS :0.9590508947148919\n",
      "VALIDATION ACCURACY :0.189697265625\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.692192467302084\n",
      "TRAINING ACCURACY :0.212265625\n",
      "VALIDATION LOSS :1.0213537774980068\n",
      "VALIDATION ACCURACY :0.158203125\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6601268707960845\n",
      "TRAINING ACCURACY :0.22642578125\n",
      "VALIDATION LOSS :1.0210394160822034\n",
      "VALIDATION ACCURACY :0.1416015625\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6343855158984661\n",
      "TRAINING ACCURACY :0.23484375\n",
      "VALIDATION LOSS :0.9528285432606936\n",
      "VALIDATION ACCURACY :0.191650390625\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6146404862403869\n",
      "TRAINING ACCURACY :0.2523046875\n",
      "VALIDATION LOSS :0.9538838109001517\n",
      "VALIDATION ACCURACY :0.183349609375\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5919295582547783\n",
      "TRAINING ACCURACY :0.26025390625\n",
      "VALIDATION LOSS :0.9582071173936129\n",
      "VALIDATION ACCURACY :0.186767578125\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5881448404490948\n",
      "TRAINING ACCURACY :0.2666796875\n",
      "VALIDATION LOSS :0.9683121386915445\n",
      "VALIDATION ACCURACY :0.186767578125\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5577667928859591\n",
      "TRAINING ACCURACY :0.28173828125\n",
      "VALIDATION LOSS :0.9420609772205353\n",
      "VALIDATION ACCURACY :0.197265625\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5486771081387997\n",
      "TRAINING ACCURACY :0.28908203125\n",
      "VALIDATION LOSS :0.9003776293247938\n",
      "VALIDATION ACCURACY :0.206787109375\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5392424192652107\n",
      "TRAINING ACCURACY :0.29439453125\n",
      "VALIDATION LOSS :0.8974228287115693\n",
      "VALIDATION ACCURACY :0.2265625\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5199234780669212\n",
      "TRAINING ACCURACY :0.3051171875\n",
      "VALIDATION LOSS :0.8837026609107852\n",
      "VALIDATION ACCURACY :0.248046875\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5079129097610712\n",
      "TRAINING ACCURACY :0.31326171875\n",
      "VALIDATION LOSS :0.9312394950538874\n",
      "VALIDATION ACCURACY :0.22314453125\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4993649735674262\n",
      "TRAINING ACCURACY :0.32052734375\n",
      "VALIDATION LOSS :0.8404146702960134\n",
      "VALIDATION ACCURACY :0.266845703125\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.49105047680437564\n",
      "TRAINING ACCURACY :0.3272265625\n",
      "VALIDATION LOSS :0.8262583771720529\n",
      "VALIDATION ACCURACY :0.30712890625\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4790992721542716\n",
      "TRAINING ACCURACY :0.33806640625\n",
      "VALIDATION LOSS :0.8426527790725231\n",
      "VALIDATION ACCURACY :0.280029296875\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.46955224078148605\n",
      "TRAINING ACCURACY :0.34384765625\n",
      "VALIDATION LOSS :0.8311897264793515\n",
      "VALIDATION ACCURACY :0.30078125\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4600664029829204\n",
      "TRAINING ACCURACY :0.34884765625\n",
      "VALIDATION LOSS :0.8541252324357629\n",
      "VALIDATION ACCURACY :0.28759765625\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4482665158063173\n",
      "TRAINING ACCURACY :0.35833984375\n",
      "VALIDATION LOSS :0.8567126858979464\n",
      "VALIDATION ACCURACY :0.29296875\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.44042402882128956\n",
      "TRAINING ACCURACY :0.365546875\n",
      "VALIDATION LOSS :0.8555259304121137\n",
      "VALIDATION ACCURACY :0.30322265625\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4318279335461557\n",
      "TRAINING ACCURACY :0.37333984375\n",
      "VALIDATION LOSS :0.8628803202882409\n",
      "VALIDATION ACCURACY :0.304443359375\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.42163054367527364\n",
      "TRAINING ACCURACY :0.38005859375\n",
      "VALIDATION LOSS :0.9033276988193393\n",
      "VALIDATION ACCURACY :0.287841796875\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4147734986618161\n",
      "TRAINING ACCURACY :0.3894921875\n",
      "VALIDATION LOSS :0.8545127082616091\n",
      "VALIDATION ACCURACY :0.31591796875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c702e2d3b51b41b1bd468236c2d64201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.038 MB uploaded\\r'), FloatProgress(value=0.10918539821892581, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▂▂▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>training_loss</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▂▂▂▄▃▄▅▄▄▅▅▅▅▅▆▆▆▆▇█▇█▇▇██▇█</td></tr><tr><td>validation_loss</td><td>█▆▄▃▄▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>30</td></tr><tr><td>training_accuracy</td><td>0.38949</td></tr><tr><td>training_loss</td><td>0.41477</td></tr><tr><td>validation_accuracy</td><td>0.31592</td></tr><tr><td>validation_loss</td><td>0.85451</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/u5qquafi' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/u5qquafi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_174344-u5qquafi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sz5uinq6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008765319780103813\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_175420-sz5uinq6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/sz5uinq6' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/sz5uinq6' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/sz5uinq6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.677952388599515\n",
      "TRAINING ACCURACY :0.061015625\n",
      "VALIDATION LOSS :1.053777546621859\n",
      "VALIDATION ACCURACY :0.110595703125\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8269407499209046\n",
      "TRAINING ACCURACY :0.16998046875\n",
      "VALIDATION LOSS :0.9219216257333755\n",
      "VALIDATION ACCURACY :0.168212890625\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6963038048520684\n",
      "TRAINING ACCURACY :0.21744140625\n",
      "VALIDATION LOSS :0.8454484278336167\n",
      "VALIDATION ACCURACY :0.224853515625\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6289326938614249\n",
      "TRAINING ACCURACY :0.25109375\n",
      "VALIDATION LOSS :0.8519281558692455\n",
      "VALIDATION ACCURACY :0.226806640625\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5720720713585615\n",
      "TRAINING ACCURACY :0.2769140625\n",
      "VALIDATION LOSS :0.8135883817449212\n",
      "VALIDATION ACCURACY :0.24267578125\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5425012899935245\n",
      "TRAINING ACCURACY :0.299609375\n",
      "VALIDATION LOSS :0.8164771143347025\n",
      "VALIDATION ACCURACY :0.247314453125\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5211306643113494\n",
      "TRAINING ACCURACY :0.315078125\n",
      "VALIDATION LOSS :0.7922597927972674\n",
      "VALIDATION ACCURACY :0.276611328125\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.48335765570402145\n",
      "TRAINING ACCURACY :0.33728515625\n",
      "VALIDATION LOSS :0.805380872450769\n",
      "VALIDATION ACCURACY :0.304443359375\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.46434018533676863\n",
      "TRAINING ACCURACY :0.35376953125\n",
      "VALIDATION LOSS :0.800135594792664\n",
      "VALIDATION ACCURACY :0.2939453125\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.447669219635427\n",
      "TRAINING ACCURACY :0.365859375\n",
      "VALIDATION LOSS :0.7879679873585701\n",
      "VALIDATION ACCURACY :0.31591796875\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.42847808104008434\n",
      "TRAINING ACCURACY :0.38193359375\n",
      "VALIDATION LOSS :0.8153104968369007\n",
      "VALIDATION ACCURACY :0.309326171875\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4027801952883601\n",
      "TRAINING ACCURACY :0.40162109375\n",
      "VALIDATION LOSS :0.8047756808809936\n",
      "VALIDATION ACCURACY :0.31396484375\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.38877984886989\n",
      "TRAINING ACCURACY :0.41333984375\n",
      "VALIDATION LOSS :0.8414209140464664\n",
      "VALIDATION ACCURACY :0.310302734375\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.38182948805391786\n",
      "TRAINING ACCURACY :0.4216796875\n",
      "VALIDATION LOSS :0.8380468273535371\n",
      "VALIDATION ACCURACY :0.326171875\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3636717557907104\n",
      "TRAINING ACCURACY :0.4373046875\n",
      "VALIDATION LOSS :0.828530521132052\n",
      "VALIDATION ACCURACY :0.325927734375\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3549824759364128\n",
      "TRAINING ACCURACY :0.44435546875\n",
      "VALIDATION LOSS :0.8445544438436627\n",
      "VALIDATION ACCURACY :0.312255859375\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3434692276269197\n",
      "TRAINING ACCURACY :0.4576171875\n",
      "VALIDATION LOSS :0.8665967313572764\n",
      "VALIDATION ACCURACY :0.30859375\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.33385814286768434\n",
      "TRAINING ACCURACY :0.46845703125\n",
      "VALIDATION LOSS :0.878603670746088\n",
      "VALIDATION ACCURACY :0.345947265625\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3187500475160778\n",
      "TRAINING ACCURACY :0.48310546875\n",
      "VALIDATION LOSS :0.8643055688589811\n",
      "VALIDATION ACCURACY :0.3232421875\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3088105356507003\n",
      "TRAINING ACCURACY :0.49298828125\n",
      "VALIDATION LOSS :0.8878735275939107\n",
      "VALIDATION ACCURACY :0.3291015625\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3028465842269361\n",
      "TRAINING ACCURACY :0.4990625\n",
      "VALIDATION LOSS :0.915928740054369\n",
      "VALIDATION ACCURACY :0.333984375\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2931655294075608\n",
      "TRAINING ACCURACY :0.508046875\n",
      "VALIDATION LOSS :0.92149643227458\n",
      "VALIDATION ACCURACY :0.348388671875\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.29225608345121146\n",
      "TRAINING ACCURACY :0.513515625\n",
      "VALIDATION LOSS :0.9278244385495782\n",
      "VALIDATION ACCURACY :0.32958984375\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.28135704902932046\n",
      "TRAINING ACCURACY :0.52375\n",
      "VALIDATION LOSS :0.9192806771025062\n",
      "VALIDATION ACCURACY :0.338134765625\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2700788376480341\n",
      "TRAINING ACCURACY :0.5355859375\n",
      "VALIDATION LOSS :0.9432951752096415\n",
      "VALIDATION ACCURACY :0.32275390625\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2659191393479705\n",
      "TRAINING ACCURACY :0.54279296875\n",
      "VALIDATION LOSS :0.9429380679503083\n",
      "VALIDATION ACCURACY :0.325439453125\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.25912388298660516\n",
      "TRAINING ACCURACY :0.55044921875\n",
      "VALIDATION LOSS :0.9774069795385003\n",
      "VALIDATION ACCURACY :0.3427734375\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2457261896505952\n",
      "TRAINING ACCURACY :0.5647265625\n",
      "VALIDATION LOSS :0.9705833364278078\n",
      "VALIDATION ACCURACY :0.342041015625\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.23947176962159575\n",
      "TRAINING ACCURACY :0.5683203125\n",
      "VALIDATION LOSS :0.9962631026282907\n",
      "VALIDATION ACCURACY :0.3447265625\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.23237387822009622\n",
      "TRAINING ACCURACY :0.58029296875\n",
      "VALIDATION LOSS :1.0393910892307758\n",
      "VALIDATION ACCURACY :0.334716796875\n",
      "\n",
      " EPOCH: 31\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2298183511570096\n",
      "TRAINING ACCURACY :0.58490234375\n",
      "VALIDATION LOSS :1.040388822555542\n",
      "VALIDATION ACCURACY :0.3330078125\n",
      "\n",
      " EPOCH: 32\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2281495696865022\n",
      "TRAINING ACCURACY :0.58943359375\n",
      "VALIDATION LOSS :1.0500231003388762\n",
      "VALIDATION ACCURACY :0.33837890625\n",
      "\n",
      " EPOCH: 33\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2217318560834974\n",
      "TRAINING ACCURACY :0.59486328125\n",
      "VALIDATION LOSS :1.0340627850964665\n",
      "VALIDATION ACCURACY :0.338623046875\n",
      "\n",
      " EPOCH: 34\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.20971264569088816\n",
      "TRAINING ACCURACY :0.6103125\n",
      "VALIDATION LOSS :1.075856052339077\n",
      "VALIDATION ACCURACY :0.346923828125\n",
      "\n",
      " EPOCH: 35\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2144794082734734\n",
      "TRAINING ACCURACY :0.60572265625\n",
      "VALIDATION LOSS :1.0753425573930144\n",
      "VALIDATION ACCURACY :0.34765625\n",
      "\n",
      " EPOCH: 36\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2071303556393832\n",
      "TRAINING ACCURACY :0.61560546875\n",
      "VALIDATION LOSS :1.0532924188300967\n",
      "VALIDATION ACCURACY :0.340576171875\n",
      "\n",
      " EPOCH: 37\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.1997169870045036\n",
      "TRAINING ACCURACY :0.62435546875\n",
      "VALIDATION LOSS :1.0793508375063539\n",
      "VALIDATION ACCURACY :0.349853515625\n",
      "\n",
      " EPOCH: 38\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.19797317830845715\n",
      "TRAINING ACCURACY :0.62826171875\n",
      "VALIDATION LOSS :1.1180866984650493\n",
      "VALIDATION ACCURACY :0.352294921875\n",
      "\n",
      " EPOCH: 39\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.19150606353767216\n",
      "TRAINING ACCURACY :0.6377734375\n",
      "VALIDATION LOSS :1.1301713697612286\n",
      "VALIDATION ACCURACY :0.34130859375\n",
      "\n",
      " EPOCH: 40\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.184291283627972\n",
      "TRAINING ACCURACY :0.64408203125\n",
      "VALIDATION LOSS :1.141298085451126\n",
      "VALIDATION ACCURACY :0.343505859375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5ae2d587584ab58fe5c5745d2945ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▂▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>training_loss</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▄▅▅▆▇▆▇▇▇▇▇▇▇▇█▇▇▇█▇█▇▇███▇▇█████████</td></tr><tr><td>validation_loss</td><td>▆▄▂▂▂▂▁▁▁▁▂▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▆▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>40</td></tr><tr><td>training_accuracy</td><td>0.64408</td></tr><tr><td>training_loss</td><td>0.18429</td></tr><tr><td>validation_accuracy</td><td>0.34351</td></tr><tr><td>validation_loss</td><td>1.1413</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/sz5uinq6' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/sz5uinq6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_175420-sz5uinq6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: deqlbkki with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0009682998704477824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RAdam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_184208-deqlbkki</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/deqlbkki' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/deqlbkki' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/deqlbkki</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :3.4515267145633697\n",
      "TRAINING ACCURACY :0.0\n",
      "VALIDATION LOSS :2.886043071746826\n",
      "VALIDATION ACCURACY :0.0\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.9999766767024993\n",
      "TRAINING ACCURACY :0.009375\n",
      "VALIDATION LOSS :1.7026739418506622\n",
      "VALIDATION ACCURACY :0.00244140625\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.4207876253128051\n",
      "TRAINING ACCURACY :0.04150390625\n",
      "VALIDATION LOSS :1.5833080038428307\n",
      "VALIDATION ACCURACY :0.002685546875\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.1938881519436837\n",
      "TRAINING ACCURACY :0.07244140625\n",
      "VALIDATION LOSS :1.4439180046319962\n",
      "VALIDATION ACCURACY :0.0166015625\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.0770558804273604\n",
      "TRAINING ACCURACY :0.0944921875\n",
      "VALIDATION LOSS :1.6932889372110367\n",
      "VALIDATION ACCURACY :0.00390625\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.9753513777256012\n",
      "TRAINING ACCURACY :0.11927734375\n",
      "VALIDATION LOSS :1.1277881786227226\n",
      "VALIDATION ACCURACY :0.080078125\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.8964692649245262\n",
      "TRAINING ACCURACY :0.14134765625\n",
      "VALIDATION LOSS :1.0572412349283695\n",
      "VALIDATION ACCURACY :0.111083984375\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7969466438889503\n",
      "TRAINING ACCURACY :0.1684765625\n",
      "VALIDATION LOSS :0.9997675642371178\n",
      "VALIDATION ACCURACY :0.167236328125\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7752547508478165\n",
      "TRAINING ACCURACY :0.18146484375\n",
      "VALIDATION LOSS :1.1493648439645767\n",
      "VALIDATION ACCURACY :0.04931640625\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7243857762217522\n",
      "TRAINING ACCURACY :0.1965625\n",
      "VALIDATION LOSS :1.0374497435986996\n",
      "VALIDATION ACCURACY :0.1279296875\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6847946773469448\n",
      "TRAINING ACCURACY :0.2140625\n",
      "VALIDATION LOSS :1.102950505912304\n",
      "VALIDATION ACCURACY :0.09130859375\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6621375338733196\n",
      "TRAINING ACCURACY :0.22580078125\n",
      "VALIDATION LOSS :1.1194478049874306\n",
      "VALIDATION ACCURACY :0.1025390625\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6563878898322583\n",
      "TRAINING ACCURACY :0.235\n",
      "VALIDATION LOSS :0.9914865829050541\n",
      "VALIDATION ACCURACY :0.131103515625\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.611568216830492\n",
      "TRAINING ACCURACY :0.25279296875\n",
      "VALIDATION LOSS :0.9069756790995598\n",
      "VALIDATION ACCURACY :0.207275390625\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6034676414728165\n",
      "TRAINING ACCURACY :0.2606640625\n",
      "VALIDATION LOSS :0.8928731754422188\n",
      "VALIDATION ACCURACY :0.23046875\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5813410192728042\n",
      "TRAINING ACCURACY :0.26798828125\n",
      "VALIDATION LOSS :0.9658611007034779\n",
      "VALIDATION ACCURACY :0.1728515625\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5727573898434639\n",
      "TRAINING ACCURACY :0.27732421875\n",
      "VALIDATION LOSS :0.8983936049044132\n",
      "VALIDATION ACCURACY :0.216552734375\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5544322395324707\n",
      "TRAINING ACCURACY :0.28783203125\n",
      "VALIDATION LOSS :0.8980697244405746\n",
      "VALIDATION ACCURACY :0.235595703125\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5379566100239753\n",
      "TRAINING ACCURACY :0.29552734375\n",
      "VALIDATION LOSS :0.8710576742887497\n",
      "VALIDATION ACCURACY :0.254638671875\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5198384214937687\n",
      "TRAINING ACCURACY :0.30451171875\n",
      "VALIDATION LOSS :0.8924842663109303\n",
      "VALIDATION ACCURACY :0.243896484375\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5296229730546475\n",
      "TRAINING ACCURACY :0.3040234375\n",
      "VALIDATION LOSS :0.9388557784259319\n",
      "VALIDATION ACCURACY :0.208740234375\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5035724392533303\n",
      "TRAINING ACCURACY :0.31890625\n",
      "VALIDATION LOSS :0.9104861095547676\n",
      "VALIDATION ACCURACY :0.224853515625\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4970034085214138\n",
      "TRAINING ACCURACY :0.32009765625\n",
      "VALIDATION LOSS :0.8633410111069679\n",
      "VALIDATION ACCURACY :0.237548828125\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4972357116639614\n",
      "TRAINING ACCURACY :0.3288671875\n",
      "VALIDATION LOSS :0.8553374521434307\n",
      "VALIDATION ACCURACY :0.25634765625\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.48409016355872153\n",
      "TRAINING ACCURACY :0.3345703125\n",
      "VALIDATION LOSS :0.8714214600622654\n",
      "VALIDATION ACCURACY :0.253173828125\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4820114934444428\n",
      "TRAINING ACCURACY :0.3369921875\n",
      "VALIDATION LOSS :0.8976708501577377\n",
      "VALIDATION ACCURACY :0.257080078125\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4578173230588436\n",
      "TRAINING ACCURACY :0.35095703125\n",
      "VALIDATION LOSS :0.8383318074047565\n",
      "VALIDATION ACCURACY :0.298095703125\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.46350662037730217\n",
      "TRAINING ACCURACY :0.3486328125\n",
      "VALIDATION LOSS :0.8640523590147495\n",
      "VALIDATION ACCURACY :0.260498046875\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.45851791873574255\n",
      "TRAINING ACCURACY :0.35203125\n",
      "VALIDATION LOSS :0.8400761783123016\n",
      "VALIDATION ACCURACY :0.29296875\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.44854568734765055\n",
      "TRAINING ACCURACY :0.36130859375\n",
      "VALIDATION LOSS :0.851930346339941\n",
      "VALIDATION ACCURACY :0.293701171875\n",
      "\n",
      " EPOCH: 31\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.43719395518302917\n",
      "TRAINING ACCURACY :0.36595703125\n",
      "VALIDATION LOSS :0.919854111969471\n",
      "VALIDATION ACCURACY :0.26171875\n",
      "\n",
      " EPOCH: 32\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4485097150504589\n",
      "TRAINING ACCURACY :0.36458984375\n",
      "VALIDATION LOSS :0.8890165202319622\n",
      "VALIDATION ACCURACY :0.29052734375\n",
      "\n",
      " EPOCH: 33\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.43674851387739183\n",
      "TRAINING ACCURACY :0.36720703125\n",
      "VALIDATION LOSS :0.8605283312499523\n",
      "VALIDATION ACCURACY :0.302734375\n",
      "\n",
      " EPOCH: 34\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4258260677754879\n",
      "TRAINING ACCURACY :0.37966796875\n",
      "VALIDATION LOSS :0.882806159555912\n",
      "VALIDATION ACCURACY :0.31103515625\n",
      "\n",
      " EPOCH: 35\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4197080972790718\n",
      "TRAINING ACCURACY :0.385390625\n",
      "VALIDATION LOSS :0.8620274923741817\n",
      "VALIDATION ACCURACY :0.314208984375\n",
      "\n",
      " EPOCH: 36\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4161779172718525\n",
      "TRAINING ACCURACY :0.3883203125\n",
      "VALIDATION LOSS :0.8673302084207535\n",
      "VALIDATION ACCURACY :0.291015625\n",
      "\n",
      " EPOCH: 37\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4177617511153221\n",
      "TRAINING ACCURACY :0.393203125\n",
      "VALIDATION LOSS :0.8928784616291523\n",
      "VALIDATION ACCURACY :0.281005859375\n",
      "\n",
      " EPOCH: 38\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.40825118623673917\n",
      "TRAINING ACCURACY :0.395859375\n",
      "VALIDATION LOSS :0.893820408731699\n",
      "VALIDATION ACCURACY :0.292724609375\n",
      "\n",
      " EPOCH: 39\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4090303279459476\n",
      "TRAINING ACCURACY :0.396484375\n",
      "VALIDATION LOSS :0.8735915645956993\n",
      "VALIDATION ACCURACY :0.3017578125\n",
      "\n",
      " EPOCH: 40\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.39535425826907156\n",
      "TRAINING ACCURACY :0.40806640625\n",
      "VALIDATION LOSS :0.9171337783336639\n",
      "VALIDATION ACCURACY :0.252197265625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b88def5173d4ac4b5927068fc37d234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>training_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▃▃▅▂▄▃▃▄▆▆▅▆▆▇▆▆▆▆▇▇▇█▇██▇▇███▇▇██▇</td></tr><tr><td>validation_loss</td><td>█▄▄▃▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>40</td></tr><tr><td>training_accuracy</td><td>0.40807</td></tr><tr><td>training_loss</td><td>0.39535</td></tr><tr><td>validation_accuracy</td><td>0.2522</td></tr><tr><td>validation_loss</td><td>0.91713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/deqlbkki' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/deqlbkki</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_184208-deqlbkki/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0dizh353 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 300\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000983339645625703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fml-pc/Assignments/Assignment_3/wandb/run-20240517_184849-0dizh353</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/0dizh353' target=\"_blank\">ch23s019</a></strong> to <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch23s019/Assignment_3_start' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/sweeps/0l5hwc4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/0dizh353' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/0dizh353</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of samples: 51200\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 24\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 22\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " Number of samples: 4096\n",
      "Source Vocab length: 27\n",
      "Target Vocab length: 131\n",
      "Max sequence length for inputs: 26\n",
      "Max sequence length for outputs: 22\n",
      "\n",
      " EPOCH: 1\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :1.2844227271527051\n",
      "TRAINING ACCURACY :0.08013671875\n",
      "VALIDATION LOSS :1.1829611472785473\n",
      "VALIDATION ACCURACY :0.112548828125\n",
      "\n",
      " EPOCH: 2\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.7997508271038533\n",
      "TRAINING ACCURACY :0.17697265625\n",
      "VALIDATION LOSS :0.9140072669833899\n",
      "VALIDATION ACCURACY :0.181396484375\n",
      "\n",
      " EPOCH: 3\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6743815407529473\n",
      "TRAINING ACCURACY :0.224453125\n",
      "VALIDATION LOSS :0.8688921993598342\n",
      "VALIDATION ACCURACY :0.213623046875\n",
      "\n",
      " EPOCH: 4\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.6197602025046944\n",
      "TRAINING ACCURACY :0.25396484375\n",
      "VALIDATION LOSS :0.8690950283780694\n",
      "VALIDATION ACCURACY :0.2236328125\n",
      "\n",
      " EPOCH: 5\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5813016069307924\n",
      "TRAINING ACCURACY :0.27859375\n",
      "VALIDATION LOSS :0.8433016194030643\n",
      "VALIDATION ACCURACY :0.2294921875\n",
      "\n",
      " EPOCH: 6\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5386668922379613\n",
      "TRAINING ACCURACY :0.29890625\n",
      "VALIDATION LOSS :0.8325944077223539\n",
      "VALIDATION ACCURACY :0.271728515625\n",
      "\n",
      " EPOCH: 7\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.5147345880791545\n",
      "TRAINING ACCURACY :0.31890625\n",
      "VALIDATION LOSS :0.814422283321619\n",
      "VALIDATION ACCURACY :0.269287109375\n",
      "\n",
      " EPOCH: 8\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.49279700715094804\n",
      "TRAINING ACCURACY :0.33474609375\n",
      "VALIDATION LOSS :0.8109273407608271\n",
      "VALIDATION ACCURACY :0.299072265625\n",
      "\n",
      " EPOCH: 9\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4622280573099852\n",
      "TRAINING ACCURACY :0.35474609375\n",
      "VALIDATION LOSS :0.9285379974171519\n",
      "VALIDATION ACCURACY :0.242919921875\n",
      "\n",
      " EPOCH: 10\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4557320560514927\n",
      "TRAINING ACCURACY :0.364609375\n",
      "VALIDATION LOSS :0.8374250056222081\n",
      "VALIDATION ACCURACY :0.3017578125\n",
      "\n",
      " EPOCH: 11\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.43187296379357576\n",
      "TRAINING ACCURACY :0.38103515625\n",
      "VALIDATION LOSS :0.8253683736547828\n",
      "VALIDATION ACCURACY :0.294677734375\n",
      "\n",
      " EPOCH: 12\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.4157489341124892\n",
      "TRAINING ACCURACY :0.39478515625\n",
      "VALIDATION LOSS :0.8555211462080479\n",
      "VALIDATION ACCURACY :0.306884765625\n",
      "\n",
      " EPOCH: 13\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3945363983884454\n",
      "TRAINING ACCURACY :0.4096484375\n",
      "VALIDATION LOSS :0.8451964035630226\n",
      "VALIDATION ACCURACY :0.311767578125\n",
      "\n",
      " EPOCH: 14\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3800759181380272\n",
      "TRAINING ACCURACY :0.42373046875\n",
      "VALIDATION LOSS :0.8546780003234744\n",
      "VALIDATION ACCURACY :0.31005859375\n",
      "\n",
      " EPOCH: 15\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.36782560128718617\n",
      "TRAINING ACCURACY :0.43234375\n",
      "VALIDATION LOSS :0.9128768881782889\n",
      "VALIDATION ACCURACY :0.30615234375\n",
      "\n",
      " EPOCH: 16\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.36167097426950934\n",
      "TRAINING ACCURACY :0.44552734375\n",
      "VALIDATION LOSS :0.885271767154336\n",
      "VALIDATION ACCURACY :0.299072265625\n",
      "\n",
      " EPOCH: 17\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.3381217870116234\n",
      "TRAINING ACCURACY :0.46189453125\n",
      "VALIDATION LOSS :0.8786278115585446\n",
      "VALIDATION ACCURACY :0.3154296875\n",
      "\n",
      " EPOCH: 18\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.32649731941521165\n",
      "TRAINING ACCURACY :0.47375\n",
      "VALIDATION LOSS :0.8925934806466103\n",
      "VALIDATION ACCURACY :0.323486328125\n",
      "\n",
      " EPOCH: 19\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.323004138097167\n",
      "TRAINING ACCURACY :0.481796875\n",
      "VALIDATION LOSS :0.9121651519089937\n",
      "VALIDATION ACCURACY :0.301513671875\n",
      "\n",
      " EPOCH: 20\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.31046156710013745\n",
      "TRAINING ACCURACY :0.4920703125\n",
      "VALIDATION LOSS :0.9546252703294158\n",
      "VALIDATION ACCURACY :0.308349609375\n",
      "\n",
      " EPOCH: 21\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.29423371467739345\n",
      "TRAINING ACCURACY :0.50828125\n",
      "VALIDATION LOSS :0.9746417747810483\n",
      "VALIDATION ACCURACY :0.321044921875\n",
      "\n",
      " EPOCH: 22\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.28392105173319576\n",
      "TRAINING ACCURACY :0.5201171875\n",
      "VALIDATION LOSS :0.9932445306330919\n",
      "VALIDATION ACCURACY :0.32275390625\n",
      "\n",
      " EPOCH: 23\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.2711157798953354\n",
      "TRAINING ACCURACY :0.5335546875\n",
      "VALIDATION LOSS :0.9870457099750638\n",
      "VALIDATION ACCURACY :0.3251953125\n",
      "\n",
      " EPOCH: 24\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.26772648259997367\n",
      "TRAINING ACCURACY :0.54291015625\n",
      "VALIDATION LOSS :0.9902917072176933\n",
      "VALIDATION ACCURACY :0.326416015625\n",
      "\n",
      " EPOCH: 25\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.25237125909887254\n",
      "TRAINING ACCURACY :0.55533203125\n",
      "VALIDATION LOSS :1.025893496349454\n",
      "VALIDATION ACCURACY :0.328857421875\n",
      "\n",
      " EPOCH: 26\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.24872986142523587\n",
      "TRAINING ACCURACY :0.564140625\n",
      "VALIDATION LOSS :1.0243904115632176\n",
      "VALIDATION ACCURACY :0.33349609375\n",
      "\n",
      " EPOCH: 27\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.24140368713065982\n",
      "TRAINING ACCURACY :0.57423828125\n",
      "VALIDATION LOSS :1.0589793203398585\n",
      "VALIDATION ACCURACY :0.34521484375\n",
      "\n",
      " EPOCH: 28\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.22967591271735727\n",
      "TRAINING ACCURACY :0.585078125\n",
      "VALIDATION LOSS :1.0813914760947227\n",
      "VALIDATION ACCURACY :0.3173828125\n",
      "\n",
      " EPOCH: 29\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.22311420536600055\n",
      "TRAINING ACCURACY :0.5953125\n",
      "VALIDATION LOSS :1.0969059942290187\n",
      "VALIDATION ACCURACY :0.333251953125\n",
      "\n",
      " EPOCH: 30\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.21464027053676546\n",
      "TRAINING ACCURACY :0.604765625\n",
      "VALIDATION LOSS :1.1267064530402422\n",
      "VALIDATION ACCURACY :0.3359375\n",
      "\n",
      " EPOCH: 31\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.21209595754742622\n",
      "TRAINING ACCURACY :0.6109765625\n",
      "VALIDATION LOSS :1.1096742721274495\n",
      "VALIDATION ACCURACY :0.329345703125\n",
      "\n",
      " EPOCH: 32\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.20073416423983872\n",
      "TRAINING ACCURACY :0.62265625\n",
      "VALIDATION LOSS :1.1530860355123878\n",
      "VALIDATION ACCURACY :0.325439453125\n",
      "\n",
      " EPOCH: 33\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.199091878850013\n",
      "TRAINING ACCURACY :0.63134765625\n",
      "VALIDATION LOSS :1.144572707824409\n",
      "VALIDATION ACCURACY :0.339111328125\n",
      "\n",
      " EPOCH: 34\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.1911759385606274\n",
      "TRAINING ACCURACY :0.6403125\n",
      "VALIDATION LOSS :1.1813526945188642\n",
      "VALIDATION ACCURACY :0.33056640625\n",
      "\n",
      " EPOCH: 35\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.1852464495971799\n",
      "TRAINING ACCURACY :0.65091796875\n",
      "VALIDATION LOSS :1.177530805580318\n",
      "VALIDATION ACCURACY :0.34375\n",
      "\n",
      " EPOCH: 36\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.18060454588383437\n",
      "TRAINING ACCURACY :0.656640625\n",
      "VALIDATION LOSS :1.164177294820547\n",
      "VALIDATION ACCURACY :0.349609375\n",
      "\n",
      " EPOCH: 37\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.17578990298323333\n",
      "TRAINING ACCURACY :0.6634375\n",
      "VALIDATION LOSS :1.2254130728542805\n",
      "VALIDATION ACCURACY :0.356689453125\n",
      "\n",
      " EPOCH: 38\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.16361164714209736\n",
      "TRAINING ACCURACY :0.67771484375\n",
      "VALIDATION LOSS :1.263127639889717\n",
      "VALIDATION ACCURACY :0.335693359375\n",
      "\n",
      " EPOCH: 39\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.16692906571552157\n",
      "TRAINING ACCURACY :0.6782421875\n",
      "VALIDATION LOSS :1.241598497144878\n",
      "VALIDATION ACCURACY :0.3466796875\n",
      "\n",
      " EPOCH: 40\n",
      "CrossEntropyLoss()\n",
      "TRAINING LOSS :0.16230189707130194\n",
      "TRAINING ACCURACY :0.68365234375\n",
      "VALIDATION LOSS :1.2412109151482582\n",
      "VALIDATION ACCURACY :0.336669921875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39a8707fa794a34a2b27b6dec2298e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_accuracy</td><td>▁▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>training_loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▄▄▆▅▆▅▆▆▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇███▇█▇</td></tr><tr><td>validation_loss</td><td>▇▃▂▂▂▁▁▁▃▁▁▂▂▂▃▂▂▂▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▆▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epochs</td><td>40</td></tr><tr><td>training_accuracy</td><td>0.68365</td></tr><tr><td>training_loss</td><td>0.1623</td></tr><tr><td>validation_accuracy</td><td>0.33667</td></tr><tr><td>validation_loss</td><td>1.24121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ch23s019</strong> at: <a href='https://wandb.ai/ch23s019/Assignment_3_start/runs/0dizh353' target=\"_blank\">https://wandb.ai/ch23s019/Assignment_3_start/runs/0dizh353</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240517_184849-0dizh353/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sweep_config)\n",
    "sweep_id = wandb.sweep(sweep_config,project=\"Assignment_3_start\")\n",
    "wandb.agent(sweep_id,function=sweep_train, count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MISC For Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calc_test(target, output, train = True):\n",
    "    Outputs, Targets = [], []\n",
    "    target = target.transpose(0, 1)\n",
    "    num_correct = 0\n",
    "    batch_size = target.shape[0]\n",
    "\n",
    "    target_indices = (target == 1).nonzero()[:, 1]\n",
    "\n",
    "    # print(target.shape, output.shape )\n",
    "    assert (batch_size == len(target_indices))\n",
    "\n",
    "    if train:\n",
    "        output = output.argmax(2) # LxB\n",
    "        output = output.transpose(0, 1)\n",
    "        output_indices = (output == 1).nonzero()[:, 1]\n",
    "        for seq, i in zip(range(batch_size), target_indices):\n",
    "            if torch.all(output[seq, :i+1] == target[seq, :i+1]):\n",
    "                num_correct += 1\n",
    "                # print(output[seq, :i+1], target[seq, :i+1])\n",
    "        return num_correct, batch_size\n",
    "\n",
    "    else:\n",
    "        output = output.argmax(2) # LxB\n",
    "        output = output.transpose(0, 1)\n",
    "        output_indices = (output == 1).nonzero()[:, 1]\n",
    "        for seq, i in zip(range(batch_size), target_indices):\n",
    "            if torch.all(output[seq, :i+1] == target[seq, :i+1]):\n",
    "                num_correct += 1\n",
    "        Outputs.append(output)\n",
    "        Targets.append(target)  \n",
    "        return num_correct, batch_size, Outputs, Targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = '/home/fml-pc/Assignments/Assignment_3/Assignment_3_2024/aksharantar_sampled/aksharantar_sampled/hin'\n",
    "dataset_func = Data_Preparation(DATAPATH)\n",
    "train_dataloader, validation_dataloader, test_dataloader = dataset_func.create_dataloaders(batch_size=128)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = dataset_func.target_char2int[\"-PAD-\"])\n",
    "int2char = dataset_func.target_int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, iterator,criterion, beam_width, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total_no_correct = 0\n",
    "    total_samples = 0\n",
    "    Final_Outputs, Final_Targets = [], []\n",
    "\n",
    "    print(criterion)\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0, to_train=True).to(device)\n",
    "            \n",
    "            trg = trg.transpose(0,1)\n",
    "\n",
    "            trg = trg.argmax(2)\n",
    "\n",
    "   \n",
    "\n",
    "            num_correct, num_samples, Outputs, Targets = accuracy_calc_test(trg, output, train=False)\n",
    "\n",
    "            Final_Outputs.extend(Outputs)\n",
    "            Final_Targets.extend(Targets)\n",
    "            \n",
    "\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = trg[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    " \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            total_no_correct += num_correct\n",
    "            total_samples += num_samples\n",
    "        # print(total_no_correct, total_samples)\n",
    "    return epoch_loss / len(iterator), total_no_correct/total_samples, Final_Outputs, Final_Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ = {\n",
    "    'method': 'bayes',\n",
    "    'metric':{\n",
    "        'name' : 'val_accuracy',\n",
    "        'goal' : 'maximize'},\n",
    "    'parameters':{\n",
    "        'epochs':{'values': 28},\n",
    "        'cell_type':{'values': 'LSTM'},\n",
    "        'n_layers':{'values': 2 },\n",
    "        'hidden_dim':{'values': 1024 },\n",
    "        'encoder_embedding_dim':{'values': 200 },\n",
    "        'dropout':{'values':0.5},\n",
    "        'teacher_forcing_ratio':{'values': 0.4},\n",
    "        'learning_rate': 0.00042645834279076306,\n",
    "        'optimizer':{'values': 'RAdam'},\n",
    "        'batch_size': {'values':128 },\n",
    "        'weight_decay':{'values':[0]},\n",
    "        'attention':{'values':True},\n",
    "        'beam_width': {'values': 1}        \n",
    "    },\n",
    "}\n",
    "\n",
    "DATAPATH = '/home/fml-pc/Assignments/Assignment_3/Assignment_3_2024/aksharantar_sampled/aksharantar_sampled/hin'\n",
    "dataset_func = Data_Preparation(DATAPATH)\n",
    "train_dataloader, validation_dataloader, test_dataloader = dataset_func.create_dataloaders(config_['parameters']['batch_size']['values'])\n",
    "\n",
    "num_encoder_tokens = dataset_func.num_encoder_tokens\n",
    "hidden_dim = config_['parameters']['hidden_dim']['values']\n",
    "n_layers = config_['parameters']['n_layers']['values']\n",
    "encoder_embedding_dim = config_['parameters']['encoder_embedding_dim']['values']\n",
    "dropout = config_['parameters']['dropout']['values']\n",
    "cell_type = config_['parameters']['cell_type']['values']\n",
    "decoder_embedding_dim = config_['parameters']['encoder_embedding_dim']['values']\n",
    "num_decoder_tokens = dataset_func.num_decoder_tokens\n",
    "\n",
    "attention = config_['parameters']['attention']['values']\n",
    "\n",
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc = Encoder(num_encoder_tokens, hidden_dim, n_layers, dropout, encoder_embedding_dim, cell_type, verbose=False)\n",
    "dec = Decoder(num_decoder_tokens, hidden_dim, n_layers, dropout, decoder_embedding_dim, cell_type, atten=attention, verbose=False)\n",
    "model = Seq2Seq(enc, dec, dataset_func.max_source_length, dataset_func.max_target_length, dataset_func.target_char2int, dataset_func.num_decoder_tokens, device)\n",
    "model = model.to(device)\n",
    "\n",
    "# model = load_model_state(model, './model_state.pth')\n",
    "\n",
    "model = torch.load('/home/fml-pc/Assignments/Assignment_3/Assignment_3_2024/model_with_attention.pth')\n",
    "loss, acc, Outputs, Targets = evaluate_test(model, test_dataloader,criterion, 1, device)\n",
    "\n",
    "\n",
    "loss, acc, Outputs, Targets = evaluate_test(model, test_dataloader,criterion, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flattened_outputs = list(itertools.chain.from_iterable(Outputs))\n",
    "flattened_targets = list(itertools.chain.from_iterable(Targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1 = dataset_func.test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hin_outputs = []\n",
    "for b, i in enumerate(flattened_outputs):\n",
    "    # Convert tensor to numpy array\n",
    "    Z = i.cpu().numpy()\n",
    "\n",
    "    try:\n",
    "        # Find the index of the first occurrence of 1\n",
    "        idx = np.where(Z == 1)[0][0]\n",
    "        # Slice the array up to the index\n",
    "        A = Z[1:idx]\n",
    "    except IndexError:\n",
    "        # If 1 is not found, handle the exception\n",
    "        print(f\"No '1' found in the array at index {b}, using the whole array except the first element.\")\n",
    "        A = Z[1:]\n",
    "\n",
    "    print(b, A)\n",
    "\n",
    "    # Convert indices to characters\n",
    "    hin_word = ''.join([int2char[int(j)] for j in A])\n",
    "    hin_outputs.append(hin_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_all = 0\n",
    "hin_targets = []\n",
    "for b, i  in enumerate(flattened_targets):\n",
    "    Z = i.cpu().numpy()\n",
    "    # print('Before', Z)\n",
    "    # if b == 1463:\n",
    "    #    A =  Z[1:]\n",
    "    # else:\n",
    "    idx = np.where(Z == 1)[0][0]\n",
    "    A = Z[1:idx]\n",
    "    # print(b, Z[1:idx])\n",
    "    hin_word = ''\n",
    "    for j in A:\n",
    "        hin_word += int2char[j]\n",
    "    hin_targets.append(hin_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for i in hin_targets:\n",
    "    idx = test_df_1.loc[test_df_1['Hindi'] == i].index[0]\n",
    "    idxs.append(idx)\n",
    "\n",
    "# Access the corresponding 'English' values using the obtained indices\n",
    "english_values = test_df_1.loc[idxs, 'English']\n",
    "e = list(english_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming english_values, hin_targets, and hin_outputs are lists\n",
    "data = list(zip(e, hin_targets, hin_outputs))\n",
    "df_preds = pd.DataFrame(data, columns=['English Values', 'Hindi Targets', 'Hindi Outputs'])\n",
    "df_preds.to_csv('/home/fml-pc/Assignments/Assignment_3/Assignment_3_2024/Test_Predictions_without_attention.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shashank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
